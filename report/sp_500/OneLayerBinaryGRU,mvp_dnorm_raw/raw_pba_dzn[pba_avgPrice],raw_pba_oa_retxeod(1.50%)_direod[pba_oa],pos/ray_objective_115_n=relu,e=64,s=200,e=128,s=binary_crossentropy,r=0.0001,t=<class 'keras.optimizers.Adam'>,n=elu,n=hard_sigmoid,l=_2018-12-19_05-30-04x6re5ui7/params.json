{
  "activation": "relu",
  "batch_size": 64,
  "epochs": 200,
  "layer1_size": 128,
  "loss": "binary_crossentropy",
  "lr": 0.0001,
  "opt": "<class 'keras.optimizers.Adam'>",
  "output_activation": "elu",
  "recurrent_activation": "hard_sigmoid",
  "stateful": false,
  "step_size": 5
}