{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "    \n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fix_path(get_cwd('test.ipynb', 'recon' +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, vectorize\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import query_df, search_df, chained_filter, benchmark\n",
    "from data.data_api import DataAPI\n",
    "from data.access_util import col_subsetters as cs\n",
    "from recon.common import dum, count_nonnan, count_nonzero, count_both\n",
    "from recon.transform import *\n",
    "from recon.filter import *\n",
    "from recon.viz import *\n",
    "from recon.corr import corr_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all raw data, filter out data before 2018 (leave 2018 for final validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['1998-01-01 01:00:00+00:00', '1998-01-01 02:00:00+00:00',\n",
      "               '1998-01-01 03:00:00+00:00', '1998-01-01 04:00:00+00:00',\n",
      "               '1998-01-01 05:00:00+00:00', '1998-01-01 06:00:00+00:00',\n",
      "               '1998-01-01 07:00:00+00:00', '1998-01-01 08:00:00+00:00',\n",
      "               '1998-01-01 09:00:00+00:00', '1998-01-01 10:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=175289, freq='H')\n",
      "DatetimeIndex(['1998-01-01 01:00:00+00:00', '1998-01-01 02:00:00+00:00',\n",
      "               '1998-01-01 03:00:00+00:00', '1998-01-01 04:00:00+00:00',\n",
      "               '1998-01-01 05:00:00+00:00', '1998-01-01 06:00:00+00:00',\n",
      "               '1998-01-01 07:00:00+00:00', '1998-01-01 08:00:00+00:00',\n",
      "               '1998-01-01 09:00:00+00:00', '1998-01-01 10:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=175289, freq='H')\n",
      "DatetimeIndex(['1998-01-01 01:00:00+00:00', '1998-01-01 02:00:00+00:00',\n",
      "               '1998-01-01 03:00:00+00:00', '1998-01-01 04:00:00+00:00',\n",
      "               '1998-01-01 05:00:00+00:00', '1998-01-01 06:00:00+00:00',\n",
      "               '1998-01-01 07:00:00+00:00', '1998-01-01 08:00:00+00:00',\n",
      "               '1998-01-01 09:00:00+00:00', '1998-01-01 10:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=175289, freq='H')\n",
      "DatetimeIndex(['1998-01-01 01:00:00+00:00', '1998-01-01 02:00:00+00:00',\n",
      "               '1998-01-01 03:00:00+00:00', '1998-01-01 04:00:00+00:00',\n",
      "               '1998-01-01 05:00:00+00:00', '1998-01-01 06:00:00+00:00',\n",
      "               '1998-01-01 07:00:00+00:00', '1998-01-01 08:00:00+00:00',\n",
      "               '1998-01-01 09:00:00+00:00', '1998-01-01 10:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=175289, freq='H')\n",
      "DatetimeIndex(['1998-01-01 01:00:00+00:00', '1998-01-01 02:00:00+00:00',\n",
      "               '1998-01-01 03:00:00+00:00', '1998-01-01 04:00:00+00:00',\n",
      "               '1998-01-01 05:00:00+00:00', '1998-01-01 06:00:00+00:00',\n",
      "               '1998-01-01 07:00:00+00:00', '1998-01-01 08:00:00+00:00',\n",
      "               '1998-01-01 09:00:00+00:00', '1998-01-01 10:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=174647, freq='H')\n",
      "DatetimeIndex(['1998-01-01 16:00:00+00:00', '1998-01-01 17:00:00+00:00',\n",
      "               '1998-01-01 18:00:00+00:00', '1998-01-01 19:00:00+00:00',\n",
      "               '1998-01-01 20:00:00+00:00', '1998-01-01 21:00:00+00:00',\n",
      "               '1998-01-01 22:00:00+00:00', '1998-01-01 23:00:00+00:00',\n",
      "               '1998-01-02 00:00:00+00:00', '1998-01-02 01:00:00+00:00',\n",
      "               ...\n",
      "               '2017-12-31 14:00:00+00:00', '2017-12-31 15:00:00+00:00',\n",
      "               '2017-12-31 16:00:00+00:00', '2017-12-31 17:00:00+00:00',\n",
      "               '2017-12-31 18:00:00+00:00', '2017-12-31 19:00:00+00:00',\n",
      "               '2017-12-31 20:00:00+00:00', '2017-12-31 21:00:00+00:00',\n",
      "               '2017-12-31 22:00:00+00:00', '2017-12-31 23:00:00+00:00'],\n",
      "              dtype='datetime64[ns, UTC]', name='id', length=174592, freq='H')\n"
     ]
    }
   ],
   "source": [
    "search_terms = {\n",
    "    'stage': 'raw'\n",
    "}\n",
    "date_range = {\n",
    "    'id': ('lt', 2018)\n",
    "}\n",
    "dfs = {}\n",
    "for rec, df in DataAPI.generate(search_terms):\n",
    "    dfs[rec.name] = df.loc[search_df(df, date_range)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dow_jones_raw_0',\n",
       " 'sp_500_raw_1',\n",
       " 'nasdaq_100_raw_2',\n",
       " 'russell_2000_raw_3',\n",
       " 'oil_raw_4',\n",
       " 'gold_raw_5']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pba = chained_filter(dfs['sp_500_raw_1'].columns, [cs['#pba']['ohlc']])\n",
    "vol = chained_filter(dfs['sp_500_raw_1'].columns, [cs['#vol']['ohlc']])\n",
    "pba_vol = pba + vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pba_open',\n",
       " 'pba_high',\n",
       " 'pba_low',\n",
       " 'pba_close',\n",
       " 'vol_open',\n",
       " 'vol_high',\n",
       " 'vol_low',\n",
       " 'vol_close']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pba_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fins = dfs['sp_500_raw_1'][pba].groupby(pd.Grouper(freq='Y'))['pba_close'].describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "quad = pd.cut(dfs['sp_500_raw_1'].index, 5)\n",
    "jan2 = dfs['sp_500_raw_1'][pba_vol].loc[quad[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "jan2['diff'] = jan2['pba_close'] - jan2['pba_open']\n",
    "jan2['pct_diff'] = jan2['diff'] / jan2['pba_close']\n",
    "\n",
    "# Prev price diff\n",
    "jan2['pv_pct_diff'] = jan2['pct_diff'].shift(1)\n",
    "\n",
    "# Prev price spread\n",
    "jan2['pv_spread'] = (jan2['pba_high'] - jan2['pba_low']).shift(1)\n",
    "\n",
    "# Prev IV\n",
    "jan2['pv_vol_diff'] = (jan2['vol_close'] - jan2['vol_open']).shift(1)\n",
    "\n",
    "# Prev IV pct_change\n",
    "jan2['pv_vol_pct_diff'] = jan2['pv_vol_diff'] / jan2['vol_close'].shift(1)\n",
    "\n",
    "# Prev IV spread\n",
    "jan2['pv_vol_spread'] = (jan2['vol_high'] - jan2['vol_low']).shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pv_pct_diff</th>\n",
       "      <td>-0.000515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv_spread</th>\n",
       "      <td>0.008056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv_vol_diff</th>\n",
       "      <td>0.003881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv_vol_pct_diff</th>\n",
       "      <td>0.000885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pv_vol_spread</th>\n",
       "      <td>0.010523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 pct_diff\n",
       "pv_pct_diff     -0.000515\n",
       "pv_spread        0.008056\n",
       "pv_vol_diff      0.003881\n",
       "pv_vol_pct_diff  0.000885\n",
       "pv_vol_spread    0.010523"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcs = ['pv_pct_diff', 'pv_spread', 'pv_vol_diff', 'pv_vol_pct_diff', 'pv_vol_spread']\n",
    "lcs = ['pct_diff']\n",
    "jan2 = jan2.dropna(axis=0, how='any', subset=fcs)\n",
    "cm = corr_mat(jan2, feat_col_name=fcs, lab_col_name=lcs)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame['pba_numBids'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(dist_probs, col_name=full_list, value=(0.01, .1, .001), showboth=False, suppress_print=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tenth_inner_prob = {} #probability value will land within .1 of the mean\n",
    "for sent in sent_list:\n",
    "    tenth_inner_prob[sent] = dist_probs(sent, .1, suppress_print=True)\n",
    "\n",
    "import operator\n",
    "sorted_tenth_prob = sorted(tenth_inner_prob.items(), key=operator.itemgetter(1))\n",
    "less_than_85_percent = list(filter(lambda x: x[1] < .85, sorted_tenth_prob))\n",
    "greater_than_85_percent = list(filter(lambda x: x[1] >= .85, sorted_tenth_prob))\n",
    "print(less_than_85_percent)\n",
    "\n",
    "less_85_stats = pd.DataFrame(columns=['sent', 'mean', 'variance'])\n",
    "lt85 = [tup[0] for tup in less_than_85_percent]\n",
    "for sent in lt85:\n",
    "    less_85_stats = less_85_stats.append({'sent':sent, 'mean':data[sent].mean(), 'variance':data[sent].var()}, ignore_index=True)\n",
    "\n",
    "greater_85_stats = pd.DataFrame(columns=['sent', 'mean', 'variance'])\n",
    "gt85 = [tup[0] for tup in greater_than_85_percent]\n",
    "for sent in gt85:\n",
    "    greater_85_stats = greater_85_stats.append({'sent':sent, 'mean':data[sent].mean(), 'variance':data[sent].var()}, ignore_index=True)\n",
    "\n",
    "less_85_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show sentiment characteristics at sizable (at least greater than 1%) moves\n",
    "move_thresh = .06\n",
    "large_moves_pos = data[data.Z_dailypct >= move_thresh]\n",
    "large_moves_neg = data[data.Z_dailypct <= -move_thresh]\n",
    "print('all rows:', len(data))\n",
    "print('all rows, no weekends:', len(data.dropna(axis=0, subset=['Z_dailydir'])))\n",
    "\n",
    "print('\\ntotal large moves:', len(large_moves_pos) + len(large_moves_neg))\n",
    "print('large up moves [greater than', str(move_thresh) +']:', len(large_moves_pos))\n",
    "print('large down moves [less than', str(-move_thresh) +']:', len(large_moves_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Compare sentiment vectors at large movement thresholds ($\\pm 6$% , $\\pm 4$% , $\\pm 2$% , $\\pm 1.5$%) to sentiment vectors of the other price movements.\n",
    "Compare things such as min/max, mean, median, variance, average probability (based on normal dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
