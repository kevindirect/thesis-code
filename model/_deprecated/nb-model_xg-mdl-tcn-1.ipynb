{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb-model_xg-mdl-tcn-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:script location: /home/kev/crunch/model/nb-model_xg-mdl.ipynb\n",
      "CRITICAL:root:using project dir: /home/kev/crunch/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from functools import partial, reduce\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "\n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'nb-model_xg-mdl.ipynb'\n",
    "dir_name = 'model'\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from dask import delayed, compute\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import torchfunc\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import MODEL_DIR, RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, is_type, pd_common_idx_rows, remove_dups_list, NestedDefaultDict, set_loglevel, search_df, chained_filter, get_variants, load_df, dump_df, load_json, gb_transpose, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, benchmark\n",
    "from common_util import isnt, window_iter, np_assert_identical_len_dim, midx_get_level, pd_rows, midx_intersect, pd_common_idx_rows, midx_split, pd_midx_to_arr, window_iter, np_at_least_nd, np_is_ndim, identity_fn\n",
    "from model.common import DATASET_DIR, XG_PROCESS_DIR, XG_DATA_DIR, XG_DIR, PYTORCH_MODELS_DIR, ERROR_CODE, TEST_RATIO, VAL_RATIO, EXPECTED_NUM_HOURS, default_dataset\n",
    "from model.common import PYTORCH_ACT_MAPPING, PYTORCH_OPT_MAPPING, PYTORCH_SCH_MAPPING, PYTORCH_LOSS_MAPPING\n",
    "from model.xg_util import xgload\n",
    "from model.train_util import pd_get_np_tvt, batchify\n",
    "from model.model_util import *\n",
    "from recon.dataset_util import GEN_GROUP_CONSTRAINTS, gen_group\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = ['sp_500', 'russell_2000', 'nasdaq_100', 'dow_jones']\n",
    "chosen_asset = assets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = xgload(XG_DATA_DIR +'features' +sep)\n",
    "l = xgload(XG_DATA_DIR +'labels' +sep)\n",
    "t = xgload(XG_DATA_DIR +'targets' +sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num f: 2520\n",
      "num l: 1008\n",
      "num t: 1504\n"
     ]
    }
   ],
   "source": [
    "print('num f: {}'.format(len(list(f))))\n",
    "print('num l: {}'.format(len(list(l))))\n",
    "print('num t: {}'.format(len(list(t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir / dret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir_pba_hoc = {a: list(l.childkeys([a, 'ddir', 'ddir', 'pba_hoc_hdxret_ddir'])) for a in assets}\n",
    "ddir_vol_hoc = {a: list(l.childkeys([a, 'ddir', 'ddir', 'vol_hoc_hdxret_ddir'])) for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dret_pba_hoc = {a: list(t.childkeys([a, 'dret', 'dret', 'pba_hoc_hdxret_dret'])) for a in assets}\n",
    "dret_vol_hoc = {a: list(t.childkeys([a, 'dret', 'dret', 'vol_hoc_hdxret_dret'])) for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir1 / dret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['lin', 'log']\n",
    "fmt3, fmt4 = '{}_{}', '{}_hdxret1_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'ddir1'\n",
    "b = 'pba_hoc'; ddir1_pba_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; ddir1_pba_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; ddir1_vol_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; ddir1_vol_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dret1'\n",
    "b = 'pba_hoc'; dret1_pba_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dret1_pba_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dret1_vol_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dret1_vol_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir2/dret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = ['0.5', '1', '2']\n",
    "stats = ['avg', 'std', 'mad', 'max', 'min']\n",
    "fmt4, fmt5 = '{}_hdxret2_{}', '{}_hdxret2({}*{},1)_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'ddir2'\n",
    "b = 'pba_hoc'; ddir2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; ddir2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; ddir2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; ddir2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dret2'\n",
    "b = 'pba_hoc'; dret2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dret2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dret2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dret2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dxfbdir1 / dxfbret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['lin', 'log']\n",
    "fmt3, fmt4 = '{}_{}', '{}_hdxcret1_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbdir1'\n",
    "b = 'pba_hoc'; dxfbdir1_pba_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dxfbdir1_pba_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dxfbdir1_vol_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dxfbdir1_vol_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbcret1'\n",
    "b = 'pba_hoc'; dxfbcret1_pba_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dxfbcret1_pba_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dxfbcret1_vol_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dxfbcret1_vol_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dxfbdir2 / dxfbcret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = ['0.5', '1', '2']\n",
    "stats = ['avg', 'std', 'mad', 'max', 'min']\n",
    "fmt4, fmt5 = '{}_hdxcret2_{}', '{}_hdxcret2({}*{},1)_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbdir2'\n",
    "b = 'pba_hoc'; dxfbdir2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dxfbdir2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dxfbdir2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dxfbdir2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbcret2'\n",
    "b = 'pba_hoc'; dxfbcret2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dxfbcret2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dxfbcret2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dxfbcret2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dc',\n",
       " 'dwrxmx',\n",
       " 'hdmx',\n",
       " 'hohlca',\n",
       " 'hdzn',\n",
       " 'hdgau',\n",
       " 'ddiff',\n",
       " 'dohlca',\n",
       " 'dwrod',\n",
       " 'dlogret',\n",
       " 'dwrpt',\n",
       " 'dffd',\n",
       " 'hdpt',\n",
       " 'dwrzn',\n",
       " 'dwrmx',\n",
       " 'hdod',\n",
       " 'hduni']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([k[1] for k in f.childkeys([assets[0]])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kc_end = ['ddiff', 'ddiff_pba_vol']\n",
    "ft_all = {a: list(f.childkeys([a, *kc_end])) for a in assets}\n",
    "feat = ft_all[chosen_asset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sp_500',\n",
       "  'ddiff',\n",
       "  'ddiff_pba_vol',\n",
       "  'pba_dohlca_ddiff',\n",
       "  'pba_dohlca_ddiff(1)'],\n",
       " ['sp_500',\n",
       "  'ddiff',\n",
       "  'ddiff_pba_vol',\n",
       "  'vol_dohlca_ddiff',\n",
       "  'vol_dohlca_ddiff(1)']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = inner_join(f[feat[0]], f[feat[1]])\n",
    "no_zero = lambda df: df[df.values.sum(axis=1) != 0]\n",
    "to_bin = lambda df: (df+1)*.5\n",
    "feature_df, label_df, target_df = pd_common_idx_rows(features_df, to_bin(no_zero(l[ddir_pba_hoc[chosen_asset][0]])), t[dret_pba_hoc[chosen_asset][0]])\n",
    "assert(feature_df.shape[0]==label_df.shape[0]==target_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ftrain, fval, ftest = map(np_at_least_nd, pd_get_np_tvt(feature_df, as_midx=False))\n",
    "#ltrain, lval, ltest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(label_df, as_midx=False))\n",
    "#ttrain, tval, ttest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(target_df, as_midx=False))\n",
    "#np_assert_identical_len_dim(ftrain, ltrain, ttrain)\n",
    "#np_assert_identical_len_dim(fval, lval, tval)\n",
    "#np_assert_identical_len_dim(ftest, ltest, ttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_par = {\n",
    "\t'epochs': 200,\n",
    "\t'batch_size': 64,\n",
    "\t'input_windows': 10,\n",
    "\t'topology': [40, 20, 10],\n",
    "\t'activation': 'elu',\n",
    "\t'opt': {\n",
    "\t\t'name': 'adam',\n",
    "\t\t'kwargs': {\n",
    "\t\t\t'lr': .001\n",
    "\t\t}\n",
    "\t},\n",
    "\t'sch': {\n",
    "\t\t'name': 'rpl',\n",
    "\t\t'kwargs': {\n",
    "\t\t\t'mode': 'min',\n",
    "\t\t\t'factor': 0.1,\n",
    "\t\t\t'patience': 10,\n",
    "\t\t\t'threshold': 0.0001,\n",
    "\t\t\t'threshold_mode': 'rel',\n",
    "\t\t\t'cooldown': 0,\n",
    "\t\t\t'min_lr': 0\n",
    "\t\t}\n",
    "\t},\n",
    "\t'loss': 'nll',\n",
    "\t'kernel_size': 10,\n",
    "\t'dropout': .8,\n",
    "\t'attention': False,\n",
    "\t'max_attn_len': 90,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNModel(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Top level Temporal CNN Classifer.\n",
    "    Note: Receptive Field Size = Number TCN Blocks * Kernel Size * Last Layer Dilation Size\n",
    "\n",
    "    Parameters:\n",
    "        input_windows (int > 0): Number of aggregation windows in the input layer\n",
    "        topology (list): Topology of the TCN divided by the window size\n",
    "        kernel_size (int > 1): CNN kernel size\n",
    "        dropout (float [0, 1]): dropout probability, probability of an element to be zeroed\n",
    "        attention (bool): whether or not to include attention block after each tcn block\n",
    "        max_attn_len (int > 0): max length of attention (only relevant if attention is set to True)\n",
    "    \"\"\"\n",
    "    def __init__(self, params, data, class_weights=None):\n",
    "        \"\"\"\n",
    "        Init method\n",
    "\n",
    "        Args:\n",
    "            params (dict): dictionary of model (hyper)parameters\n",
    "            data (tuple): tuple of pd.DataFrames\n",
    "            class_weights (dict): class weighting scheme\n",
    "        \"\"\"\n",
    "        # init superclass\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.params = params\n",
    "        #self.batch_size = params['batch_size']\n",
    "        loss_fn = PYTORCH_LOSS_MAPPING.get(self.params['loss'])\n",
    "        self.loss = loss_fn() if (isnt(class_weights)) else loss_fn(weight=class_weights)\n",
    "        ## if you specify an example input, the summary will show input/output for each layer\n",
    "        #self.example_input_array = torch.rand(5, 28 * 28)\n",
    "        self.__setup_data__(data)\n",
    "        self.__build_model__()\n",
    "\n",
    "    def __build_model__(self):\n",
    "        \"\"\"\n",
    "        TCN Based Network\n",
    "\n",
    "        Args:\n",
    "            activation (str): hidden activations to use\n",
    "            num_input_channels (int): number of input channels\n",
    "            channels (list): list of output channel sizes in order from first to last\n",
    "            num_outputs (int): number of outputs, usually the number of classes to predict (defaults to binary case)\n",
    "            kernel_size (int > 1): CNN kernel size\n",
    "            dropout (float [0, 1]): dropout probability, probability of an element to be zeroed during training\n",
    "            attention (bool): whether or not to include attention block after each tcn block\n",
    "            max_attn_len (int > 0): max length of attention (only relevant if attention is set to True)\n",
    "        \"\"\"\n",
    "        input_channels, window_size = self.feat_shape[-2:]\n",
    "        eff_history = window_size * self.params['input_windows']\t\t\t\t\t\t# Effective history = window_size * input_windows\n",
    "        scaled_topology = window_size * np.array(self.params['topology'])\t\t\t\t# Scale topology by the window size\n",
    "        channels = np.clip(scaled_topology, a_min=1, a_max=None).astype(int).tolist()\t# Make sure layer outputs are always greater than zero\n",
    "        self.tcn = TemporalConvNet(PYTORCH_ACT_MAPPING.get(self.params['activation']), num_input_channels=input_channels,\n",
    "            channels=channels, kernel_size=self.params['kernel_size'], dropout=self.params['dropout'], attention=False, max_attn_len=100)\n",
    "        self.out = nn.Linear(channels[-1], self.num_outputs)\n",
    "        self.logprob = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Input must have have shape (N, C_in, L_in) where\n",
    "            N: number of batches\n",
    "            C_in: number of input channels\n",
    "            L_in: length of input sequence\n",
    "\n",
    "        Output shape will be (N, C_out) where\n",
    "            N: number of batches\n",
    "            C_out: number of classes\n",
    "        \"\"\"\n",
    "        out_embedding = self.tcn(x)\n",
    "        out_score = self.out(out_embedding[:, :, -1])\n",
    "        out_prob = self.logprob(out_score)\n",
    "        return out_prob\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        \"\"\"\n",
    "        x, y = batch[:2]\n",
    "        y_hat = self.forward(x)\n",
    "        loss_val = self.loss(y_hat, y)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if (self.trainer.use_dp or self.trainer.use_ddp2):\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "\n",
    "        tqdm_dict = {'train_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "\n",
    "        return output # can also return a scalar (loss val) instead of a dict \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the validation loop\n",
    "        \"\"\"\n",
    "        x, y = batch[:2]\n",
    "        y_hat = self.forward(x)\n",
    "        loss_val = self.loss(y_hat, y)\n",
    "\n",
    "        # acc\n",
    "        labels_hat = torch.argmax(y_hat, dim=1)\n",
    "        val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n",
    "        val_acc = torch.tensor(val_acc)\n",
    "\n",
    "        if (self.on_gpu):\n",
    "            val_acc = val_acc.cuda(loss_val.device.index)\n",
    "\n",
    "        # in DP mode (default) make sure if result is scalar, there's another dim in the beginning\n",
    "        if (self.trainer.use_dp or self.trainer.use_ddp2):\n",
    "            loss_val = loss_val.unsqueeze(0)\n",
    "            val_acc = val_acc.unsqueeze(0)\n",
    "\n",
    "        output = OrderedDict({\n",
    "            'val_loss': loss_val,\n",
    "            'val_acc': val_acc,\n",
    "        })\n",
    "\n",
    "        return output # can also return a scalar (loss val) instead of a dict \n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        \"\"\"\n",
    "        Called at the end of validation to aggregate outputs\n",
    "        :param outputs: list of individual outputs of each validation step\n",
    "        \"\"\"\n",
    "        # if returned a scalar from validation_step, outputs is a list of tensor scalars\n",
    "        # we return just the average in this case (if we want)\n",
    "        # return torch.stack(outputs).mean()\n",
    "\n",
    "        val_loss_mean = 0\n",
    "        val_acc_mean = 0\n",
    "        for output in outputs:\n",
    "            val_loss = output['val_loss']\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            if (self.trainer.use_dp or self.trainer.use_ddp2):\n",
    "                val_loss = torch.mean(val_loss)\n",
    "            val_loss_mean += val_loss\n",
    "\n",
    "            # reduce manually when using dp\n",
    "            val_acc = output['val_acc']\n",
    "            if (self.trainer.use_dp or self.trainer.use_ddp2):\n",
    "                val_acc = torch.mean(val_acc)\n",
    "\n",
    "            val_acc_mean += val_acc\n",
    "\n",
    "        val_loss_mean /= len(outputs)\n",
    "        val_acc_mean /= len(outputs)\n",
    "        tqdm_dict = {'val_loss': val_loss_mean, 'val_acc': val_acc_mean}\n",
    "        result = {'progress_bar': tqdm_dict, 'log': tqdm_dict, 'val_loss': val_loss_mean}\n",
    "        return result\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        construct and return optimizers\n",
    "        \"\"\"\n",
    "        opt_fn = PYTORCH_OPT_MAPPING.get(self.params['opt']['name'])\n",
    "        opt = opt_fn(self.parameters(), **self.params['opt']['kwargs'])\n",
    "        return opt\n",
    "        #sch_fn = PYTORCH_SCH_MAPPING.get(self.params['sch']['name'])\n",
    "        #sch = sch_fn(opt, **self.params['sch']['kwargs'])\n",
    "        #return [opt], [sch]\n",
    "\n",
    "    def __setup_data__(self, data):\n",
    "        feature_df, label_df, target_df = data\n",
    "        ftrain, fval, ftest = map(np_at_least_nd, pd_get_np_tvt(feature_df, as_midx=False))\n",
    "        ltrain, lval, ltest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(label_df, as_midx=False))\n",
    "        ttrain, tval, ttest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(target_df, as_midx=False))\n",
    "        self.flt_train = (ftrain, ltrain, ttrain)\n",
    "        self.flt_val = (fval, lval, tval)\n",
    "        self.flt_test = (ftest, ltest, ttest)\n",
    "        np_assert_identical_len_dim(*self.flt_train)\n",
    "        np_assert_identical_len_dim(*self.flt_val)\n",
    "        np_assert_identical_len_dim(*self.flt_test)\n",
    "        # Needed for model building:\n",
    "        self.feat_shape = ftrain.shape\n",
    "        self.num_outputs = max(map(lambda a: len(np.unique(a)), (ltrain, lval, ltest)))\n",
    "\n",
    "    def preproc(self, params, data):\n",
    "        \"\"\"\n",
    "        Reshaping transform for temporal data. All data must have same number of dimensions and observations.\n",
    "\n",
    "        Runs a \"moving window unstack\" operation through the first data such that each row of the result contains the history\n",
    "        of the original up to and including that row based on a input_windows parameter in params. The input_windows\n",
    "        determines how far back the history each row will record; a input_windows of '1' results in no change.\n",
    "        This method also adds a singleton dimension between the first and second after the moving window unstack; this is to\n",
    "        denote the \"number of channels\" for CNN based learning algorithms.\n",
    "\n",
    "        example with input_windows of '2':\n",
    "                                                    0 | a b c\n",
    "                                                    1 | d e f ---> 1 | a b c d e f\n",
    "                                                    2 | g h i      2 | d e f g h i\n",
    "                                                    3 | j k l      3 | g h i j k l\n",
    "\n",
    "        All data after the first tuple item are assumed to be label/target vectors and are reshaped to align with the new first\n",
    "        tuple item.\n",
    "\n",
    "        Args:\n",
    "            params (dict): params dictionary\n",
    "            data (tuple): tuple of numpy data with features as first element\n",
    "\n",
    "        Returns:\n",
    "            Tuple of reshaped data\n",
    "        \"\"\"\n",
    "        np_assert_identical_len_dim(*data)\n",
    "        # Reshape features into overlapping moving window samples\n",
    "        f = np.array([np.concatenate(vec, axis=-1) for vec in window_iter(data[0], n=params['input_windows'])])\n",
    "        rest = [vec[params['input_windows']-1:] for vec in data[1:]]  # Realign by dropping observations prior to the first step\n",
    "        np_assert_identical_len_dim(f, *rest)\n",
    "        return (f, *rest)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        logging.info('train_dataloader called')\n",
    "        return batchify(self.params, self.preproc(self.params, self.flt_train), False)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        logging.info('val_dataloader called')\n",
    "        return batchify(self.params, self.preproc(self.params, self.flt_val), False)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        logging.info('test_dataloader called')\n",
    "        return batchify(self.params, self.preproc(self.params, self.flt_test), False)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser, root_dir):  # pragma: no cover\n",
    "        \"\"\"\n",
    "        Parameters you define here will be available to your model through self.params\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = test_par\n",
    "mdl = TCNModel(params, (feature_df, label_df, target_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCNModel(\n",
       "  (loss): NLLLoss()\n",
       "  (tcn): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(10, 40, kernel_size=(10,), stride=(1,), padding=(9,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (act1): ELU(alpha=1.0)\n",
       "        (dropout1): Dropout(p=0.8, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(10, 40, kernel_size=(10,), stride=(1,), padding=(9,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ELU(alpha=1.0)\n",
       "          (3): Dropout(p=0.8, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(10, 40, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (1): TemporalBlock(\n",
       "        (conv1): Conv1d(40, 20, kernel_size=(10,), stride=(1,), padding=(18,), dilation=(2,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (act1): ELU(alpha=1.0)\n",
       "        (dropout1): Dropout(p=0.8, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(40, 20, kernel_size=(10,), stride=(1,), padding=(18,), dilation=(2,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ELU(alpha=1.0)\n",
       "          (3): Dropout(p=0.8, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(40, 20, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (2): TemporalBlock(\n",
       "        (conv1): Conv1d(20, 10, kernel_size=(10,), stride=(1,), padding=(36,), dilation=(4,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (act1): ELU(alpha=1.0)\n",
       "        (dropout1): Dropout(p=0.8, inplace=False)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(20, 10, kernel_size=(10,), stride=(1,), padding=(36,), dilation=(4,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ELU(alpha=1.0)\n",
       "          (3): Dropout(p=0.8, inplace=False)\n",
       "        )\n",
       "        (downsample): Conv1d(20, 10, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (logprob): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tcn_setup_data(data):\n",
    "        feature_df, label_df, target_df = data\n",
    "        ftrain, fval, ftest = map(np_at_least_nd, pd_get_np_tvt(feature_df, as_midx=False))\n",
    "        ltrain, lval, ltest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(label_df, as_midx=False))\n",
    "        ttrain, tval, ttest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(target_df, as_midx=False))\n",
    "        return ftrain, ltrain, ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(params, data):\n",
    "    \"\"\"\n",
    "    Reshaping transform for temporal data. All data must have same number of dimensions and observations.\n",
    "\n",
    "    Runs a \"moving window unstack\" operation through the first data such that each row of the result contains the history\n",
    "    of the original up to and including that row based on a input_windows parameter in params. The input_windows\n",
    "    determines how far back the history each row will record; a input_windows of '1' results in no change.\n",
    "    This method also adds a singleton dimension between the first and second after the moving window unstack; this is to\n",
    "    denote the \"number of channels\" for CNN based learning algorithms.\n",
    "\n",
    "    example with input_windows of '2':\n",
    "                                                0 | a b c\n",
    "                                                1 | d e f ---> 1 | a b c d e f\n",
    "                                                2 | g h i      2 | d e f g h i\n",
    "                                                3 | j k l      3 | g h i j k l\n",
    "\n",
    "    All data after the first tuple item are assumed to be label/target vectors and are reshaped to align with the new first\n",
    "    tuple item.\n",
    "\n",
    "    Args:\n",
    "        params (dict): params dictionary\n",
    "        data (tuple): tuple of numpy data with features as first element\n",
    "\n",
    "    Returns:\n",
    "        Tuple of reshaped data\n",
    "    \"\"\"\n",
    "    np_assert_identical_len_dim(*data)\n",
    "    # Reshape features into overlapping moving window samples\n",
    "    f = np.array([np.concatenate(vec, axis=-1) for vec in window_iter(data[0], n=params['input_windows'])])\n",
    "    rest = [vec[params['input_windows']-1:] for vec in data[1:]]  # Realign by dropping observations prior to the first step\n",
    "    np_assert_identical_len_dim(f, *rest)\n",
    "    return (f, *rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = preproc(params, tcn_setup_data((feature_df, label_df, target_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1337, 10, 20)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-b11407939f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "d[0][0][0][:, :, :-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-471ce8e34a41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchomp_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mchomp_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "chomp_size = 10\n",
    "x[:, :, :-chomp_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: True, used: True\n",
      "VISIBLE GPUS: 0\n",
      "using 16bit precision\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Name             Type Params\n",
      "0                       loss          NLLLoss    0  \n",
      "1                        tcn  TemporalConvNet   15 K\n",
      "2                tcn.network       Sequential   15 K\n",
      "3              tcn.network.0    TemporalBlock    4 K\n",
      "4        tcn.network.0.conv1           Conv1d    4 K\n",
      "5       tcn.network.0.chomp1          Chomp1d    0  \n",
      "6         tcn.network.0.act1              ELU    0  \n",
      "7     tcn.network.0.dropout1          Dropout    0  \n",
      "8          tcn.network.0.net       Sequential    4 K\n",
      "9   tcn.network.0.downsample           Conv1d  440  \n",
      "10        tcn.network.0.relu             ReLU    0  \n",
      "11             tcn.network.1    TemporalBlock    8 K\n",
      "12       tcn.network.1.conv1           Conv1d    8 K\n",
      "13      tcn.network.1.chomp1          Chomp1d    0  \n",
      "14        tcn.network.1.act1              ELU    0  \n",
      "15    tcn.network.1.dropout1          Dropout    0  \n",
      "16         tcn.network.1.net       Sequential    8 K\n",
      "17  tcn.network.1.downsample           Conv1d  820  \n",
      "18        tcn.network.1.relu             ReLU    0  \n",
      "19             tcn.network.2    TemporalBlock    2 K\n",
      "20       tcn.network.2.conv1           Conv1d    2 K\n",
      "21      tcn.network.2.chomp1          Chomp1d    0  \n",
      "22        tcn.network.2.act1              ELU    0  \n",
      "23    tcn.network.2.dropout1          Dropout    0  \n",
      "24         tcn.network.2.net       Sequential    2 K\n",
      "25  tcn.network.2.downsample           Conv1d  210  \n",
      "26        tcn.network.2.relu             ReLU    0  \n",
      "27                       out           Linear   22  \n",
      "28                   logprob       LogSoftmax    0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [00:00<00:03,  1.32it/s, batch_nb=9, epoch=0, gpu=0, loss=4.367, train_loss=2.2, v_nb=5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 43.07it/s, batch_nb=10, epoch=199, gpu=0, loss=0.610, train_loss=0.515, v_nb=5, val_acc=0.491, val_loss=0.785]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 15/15 [00:12<00:00, 43.07it/s, batch_nb=10, epoch=199, gpu=0, loss=0.610, train_loss=0.515, v_nb=5, val_acc=0.491, val_loss=0.785]"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_nb_epochs=params['epochs'], gpus=1, amp_level='O2', use_amp=True)\n",
    "trainer.fit(mdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYTORCH_ACT_MAPPING, PYTORCH_OPT_MAPPING, PYTORCH_LOSS_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torchfunc.performance.tips(mdl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
