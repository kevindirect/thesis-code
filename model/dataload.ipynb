{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:script location: /home/kevin/crunch/model/dataload.ipynb\n",
      "WARNING:root:using project dir: /home/kevin/crunch/\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "    \n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'dataload.ipynb'   # FILL\n",
    "dir_name = 'model'         # FILL\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import delayed, compute\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, set_loglevel, chained_filter, get_variants, dump_df, load_json, gb_transpose, reindex_on_time_mask, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, benchmark\n",
    "from model.common import DATASET_DIR, EXPECTED_NUM_HOURS, default_dataset\n",
    "from model.data_util import datagen, prepare_transpose_data, prepare_label_data, prepare_target_data\n",
    "from data.data_api import DataAPI\n",
    "from data.access_util import df_getters as dg, col_subsetters2 as cs2\n",
    "from recon.dataset_util import prep_dataset, gen_group\n",
    "from recon.split_util import get_train_test_split, gen_time_series_split, index_three_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loglevel('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dataset: 2 mvp_dnorm_raw_pba_avgprice.json df(s)\n",
      "INFO:root:assets: sp_500\n"
     ]
    }
   ],
   "source": [
    "dataset_name = default_dataset\n",
    "assets_str = 'sp_500'\n",
    "assets = list(map(str.strip, assets_str.split(',')))\n",
    "\n",
    "dataset_dict = load_json(dataset_name, dir_path=DATASET_DIR)\n",
    "dataset = prep_dataset(dataset_dict, assets=assets)\n",
    "\n",
    "logging.info('dataset: {} {} df(s)'.format(len(dataset['features']), dataset_name))\n",
    "logging.info('assets: {}'.format(str('all' if (assets==None) else ', '.join(assets))))\n",
    "logging.debug('fpaths: {}'.format(str(list(dataset['features']['dfs'].keys()))))\n",
    "logging.debug('lpaths: {}'.format(str(list(dataset['labels']['dfs'].keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:(X, y, z) -> (raw_pba_dmx, raw_pba_oa_retxeod_direod, raw_pba_oa_retxeod_reteod)\n",
      "INFO:root:(X, y, z) -> (pba_avgPrice, pba_oa, pba_oa)\n"
     ]
    }
   ],
   "source": [
    "for i, (fpath, lpath, tpath, frec, lrec, trec, fcol, lcol, tcol, feature, label, target) in enumerate(datagen(dataset, feat_prep_fn=prepare_transpose_data, label_prep_fn=prepare_label_data, target_prep_fn=prepare_target_data, how='ser_to_ser')):\n",
    "    logging.info('(X, y, z) -> ({fdesc}, {ldesc}, {tdesc})'.format(fdesc=frec.desc, ldesc=lrec.desc, tdesc=trec.desc))\n",
    "    logging.info('(X, y, z) -> ({fcol}, {lcol}, {tcol})'.format(fcol=fcol, lcol=lcol, tcol=tcol))\n",
    "    f = feature\n",
    "    l = label\n",
    "    t = target\n",
    "    break\n",
    "#     print(feature)\n",
    "#     print(label)\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx, test_idx = index_three_split(f.index, l.index, t.index, val_ratio=.2, test_ratio=.2, shuffle=False)\n",
    "feat_train, feat_val, feat_test = f.loc[train_idx].values, f.loc[val_idx].values, f.loc[test_idx].values\n",
    "lab_train, lab_val, lab_test = l.loc[train_idx].values, l.loc[val_idx].values, l.loc[test_idx].values\n",
    "tar_train, tar_val, tar_test = t.loc[train_idx].values, t.loc[val_idx].values, t.loc[test_idx].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
