{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:script location: /home/kevin/crunch/model/dataload.ipynb\n",
      "WARNING:root:using project dir: /home/kevin/crunch/\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "    \n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'dataload.ipynb'   # FILL\n",
    "dir_name = 'model'         # FILL\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dask import delayed, compute\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, set_loglevel, chained_filter, get_variants, dump_df, load_json, gb_transpose, reindex_on_time_mask, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, pd_to_pytorch, list_get_dict, window_iter, benchmark\n",
    "from model.common import DATASET_DIR, EXPECTED_NUM_HOURS, default_dataset\n",
    "from model.data_util import datagen, prepare_transpose_data, prepare_label_data, prepare_target_data\n",
    "from data.data_api import DataAPI\n",
    "from data.access_util import df_getters as dg, col_subsetters2 as cs2\n",
    "from recon.dataset_util import prep_dataset, gen_group\n",
    "from recon.split_util import get_train_test_split, gen_time_series_split, index_three_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loglevel('info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:dataset: 2 mvp_dnorm_raw_pba_avgprice.json df(s)\n",
      "INFO:root:assets: sp_500\n"
     ]
    }
   ],
   "source": [
    "dataset_name = default_dataset\n",
    "assets_str = 'sp_500'\n",
    "assets = list(map(str.strip, assets_str.split(',')))\n",
    "\n",
    "dataset_dict = load_json(dataset_name, dir_path=DATASET_DIR)\n",
    "dataset = prep_dataset(dataset_dict, assets=assets)\n",
    "\n",
    "logging.info('dataset: {} {} df(s)'.format(len(dataset['features']), dataset_name))\n",
    "logging.info('assets: {}'.format(str('all' if (assets==None) else ', '.join(assets))))\n",
    "logging.debug('fpaths: {}'.format(str(list(dataset['features']['dfs'].keys()))))\n",
    "logging.debug('lpaths: {}'.format(str(list(dataset['labels']['dfs'].keys()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:(X, y, z) -> (raw_pba_dmx, raw_pba_oa_retxeod_direod, raw_pba_oa_retxeod_reteod)\n",
      "INFO:root:(X, y, z) -> (pba_avgPrice, pba_oa, pba_oa)\n"
     ]
    }
   ],
   "source": [
    "for i, (fpath, lpath, tpath, frec, lrec, trec, fcol, lcol, tcol, feature, label, target) in enumerate(datagen(dataset, feat_prep_fn=prepare_transpose_data, label_prep_fn=prepare_label_data, target_prep_fn=prepare_target_data, how='ser_to_ser')):\n",
    "    logging.info('(X, y, z) -> ({fdesc}, {ldesc}, {tdesc})'.format(fdesc=frec.desc, ldesc=lrec.desc, tdesc=trec.desc))\n",
    "    logging.info('(X, y, z) -> ({fcol}, {lcol}, {tcol})'.format(fcol=fcol, lcol=lcol, tcol=tcol))\n",
    "    f = feature\n",
    "    l = label\n",
    "    t = target\n",
    "    break\n",
    "#     print(feature)\n",
    "#     print(label)\n",
    "#     print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx, test_idx = index_three_split(f.index, l.index, t.index, val_ratio=.2, test_ratio=.2, shuffle=False)\n",
    "feat_train, feat_val, feat_test = f.loc[train_idx], f.loc[val_idx], f.loc[test_idx]\n",
    "lab_train, lab_val, lab_test = l.loc[train_idx], l.loc[val_idx], l.loc[test_idx]\n",
    "tar_train, tar_val, tar_test = t.loc[train_idx], t.loc[val_idx], t.loc[test_idx]\n",
    "\n",
    "feat_train_np, feat_val_np, feat_test_np = f.loc[train_idx].values, f.loc[val_idx].values, f.loc[test_idx].values\n",
    "lab_train_np, lab_val_np, lab_test_np = l.loc[train_idx].values, l.loc[val_idx].values, l.loc[test_idx].values\n",
    "tar_train_np, tar_val_np, tar_test_np = t.loc[train_idx].values, t.loc[val_idx].values, t.loc[test_idx].values\n",
    "\n",
    "feat_train_tor, feat_val_tor, feat_test_tor = map(pd_to_pytorch, (f.loc[train_idx], f.loc[val_idx], f.loc[test_idx]))\n",
    "lab_train_tor, lab_val_tor, lab_test_tor = map(pd_to_pytorch, (l.loc[train_idx], l.loc[val_idx], l.loc[test_idx]))\n",
    "tar_train_tor, tar_val_tor, tar_test_tor = map(pd_to_pytorch, (t.loc[train_idx], t.loc[val_idx], t.loc[test_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_ds = lambda *data: TensorDataset(*[torch.tensor(d) for d in data])\n",
    "data_tuples = zip([feat_train_np, feat_val_np, feat_test_np], [lab_train_np, lab_val_np, lab_test_np])\n",
    "train_ds, val_ds, test_ds = [to_ds(X, y) for X, y in data_tuples]\n",
    "train_dl, val_dl, test_dl = map(partial(DataLoader, batch_size=10, shuffle=False), [train_ds, val_ds, test_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
