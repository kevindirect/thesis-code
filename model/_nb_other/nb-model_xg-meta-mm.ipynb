{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nb-model_xg-meta-mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:script location: /home/kev/crunch/model/nb-model_xg-meta.ipynb\n",
      "CRITICAL:root:using project dir: /home/kev/crunch/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from functools import partial, reduce\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "\n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'nb-model_xg-meta.ipynb'\n",
    "dir_name = 'model'\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "from dask import delayed, compute\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import Dataset as TorchDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import torchfunc\n",
    "from torchmeta.utils.data import BatchMetaDataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import MODEL_DIR, RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, is_type, pd_common_idx_rows, remove_dups_list, NestedDefaultDict, set_loglevel, search_df, chained_filter, get_variants, load_df, dump_df, load_json, gb_transpose, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, benchmark\n",
    "from common_util import isnt, window_iter, np_assert_identical_len_dim, midx_get_level, pd_rows, midx_intersect, pd_common_idx_rows, midx_split, pd_midx_to_arr, window_iter, np_at_least_nd, np_is_ndim, identity_fn\n",
    "from model.common import DATASET_DIR, XG_PROCESS_DIR, XG_DATA_DIR, XG_DIR, PYTORCH_MODELS_DIR, ERROR_CODE, TEST_RATIO, VAL_RATIO, EXPECTED_NUM_HOURS, default_dataset\n",
    "from model.common import PYTORCH_ACT_MAPPING, PYTORCH_OPT_MAPPING, PYTORCH_SCH_MAPPING, PYTORCH_LOSS_MAPPING\n",
    "from model.xg_util import xgload\n",
    "from model.preproc_util import temporal_preproc, stride_preproc\n",
    "from model.train_util import pd_get_np_tvt, batchify\n",
    "from model.pl_util import TCNModel\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets = ['sp_500', 'russell_2000', 'nasdaq_100', 'dow_jones']\n",
    "chosen_asset = assets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = xgload(XG_DATA_DIR +'features' +sep)\n",
    "l = xgload(XG_DATA_DIR +'labels' +sep)\n",
    "t = xgload(XG_DATA_DIR +'targets' +sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num f: 2520\n",
      "num l: 1008\n",
      "num t: 1504\n"
     ]
    }
   ],
   "source": [
    "print('num f: {}'.format(len(list(f))))\n",
    "print('num l: {}'.format(len(list(l))))\n",
    "print('num t: {}'.format(len(list(t))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir / dret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir_pba_hoc = {a: list(l.childkeys([a, 'ddir', 'ddir', 'pba_hoc_hdxret_ddir'])) for a in assets}\n",
    "ddir_vol_hoc = {a: list(l.childkeys([a, 'ddir', 'ddir', 'vol_hoc_hdxret_ddir'])) for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dret_pba_hoc = {a: list(t.childkeys([a, 'dret', 'dret', 'pba_hoc_hdxret_dret'])) for a in assets}\n",
    "dret_vol_hoc = {a: list(t.childkeys([a, 'dret', 'dret', 'vol_hoc_hdxret_dret'])) for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir1 / dret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['lin', 'log']\n",
    "fmt3, fmt4 = '{}_{}', '{}_hdxret1_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'ddir1'\n",
    "b = 'pba_hoc'; ddir1_pba_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; ddir1_pba_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; ddir1_vol_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; ddir1_vol_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dret1'\n",
    "b = 'pba_hoc'; dret1_pba_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dret1_pba_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dret1_vol_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dret1_vol_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ddir2/dret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = ['0.5', '1', '2']\n",
    "stats = ['avg', 'std', 'mad', 'max', 'min']\n",
    "fmt4, fmt5 = '{}_hdxret2_{}', '{}_hdxret2({}*{},1)_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'ddir2'\n",
    "b = 'pba_hoc'; ddir2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; ddir2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; ddir2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; ddir2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dret2'\n",
    "b = 'pba_hoc'; dret2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dret2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dret2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dret2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dxfbdir1 / dxfbret1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['lin', 'log']\n",
    "fmt3, fmt4 = '{}_{}', '{}_hdxcret1_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbdir1'\n",
    "b = 'pba_hoc'; dxfbdir1_pba_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dxfbdir1_pba_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dxfbdir1_vol_hoc = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dxfbdir1_vol_hlh = {a: {g: list(l.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbcret1'\n",
    "#fmt3, fmt4 = '{}_{}', '{}_hdxcret1_{}'\n",
    "b = 'pba_hoc'; dxfbcret1_pba_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dxfbcret1_pba_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dxfbcret1_vol_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dxfbcret1_vol_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbval1'\n",
    "b = 'pba_hoc'; dxfbval1_pba_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'pba_hlh'; dxfbval1_pba_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hoc'; dxfbval1_vol_hoc = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}\n",
    "b = 'vol_hlh'; dxfbval1_vol_hlh = {a: {g: list(t.childkeys([a, e, fmt3.format(e, g), fmt4.format(b, e)])) for g in groups} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dxfbdir2 / dxfbcret2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalars = ['0.5', '1', '2']\n",
    "stats = ['avg', 'std', 'mad', 'max', 'min']\n",
    "fmt4, fmt5 = '{}_hdxcret2_{}', '{}_hdxcret2({}*{},1)_{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbdir2'\n",
    "b = 'pba_hoc'; dxfbdir2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dxfbdir2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dxfbdir2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dxfbdir2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbcret2'\n",
    "b = 'pba_hoc'; dxfbcret2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dxfbcret2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dxfbcret2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dxfbcret2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = 'dxfbval2'\n",
    "b = 'pba_hoc'; dxfbval2_pba_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'pba_hlh'; dxfbval2_pba_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hoc'; dxfbval2_vol_hoc = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}\n",
    "b = 'vol_hlh'; dxfbval2_vol_hlh = {a: {d: [[a, e, e, fmt4.format(b, e), fmt5.format(b, c, d, e)] for c in scalars] for d in stats} for a in assets}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Labels/Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lt(d, store, asset=chosen_asset, subset=None):\n",
    "    \"\"\"\n",
    "    Return label or target data as a DataFrame.\n",
    "    \"\"\"\n",
    "    kcs = d[asset][subset] if (subset) else d[asset]\n",
    "    lt = pd.concat([store[kc] for kc in kcs], axis=1, keys=[kc[-1] for kc in kcs])\n",
    "    lt.columns = lt.columns.droplevel(-1)\n",
    "    return lt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir = get_lt(ddir_pba_hoc, l)\n",
    "ddir1 = get_lt(ddir1_pba_hoc, l, subset='log')\n",
    "dxfbdir1 = get_lt(dxfbdir1_pba_hoc, l, subset='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dret = get_lt(dret_pba_hoc, t)\n",
    "dret1 = get_lt(dret1_pba_hoc, t, subset='log')\n",
    "dxfbcret1 = get_lt(dxfbcret1_pba_hoc, t, subset='log')\n",
    "dxfbval1 = get_lt(dxfbval1_pba_hoc, t, subset='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddir_vol = get_lt(ddir_vol_hoc, l)\n",
    "ddir1_vol = get_lt(ddir1_vol_hoc, l, subset='log')\n",
    "dxfbdir1_vol = get_lt(dxfbdir1_vol_hoc, l, subset='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dret_vol = get_lt(dret_vol_hoc, t)\n",
    "dret1_vol = get_lt(dret1_vol_hoc, t, subset='log')\n",
    "dxfbcret1_vol = get_lt(dxfbcret1_vol_hoc, t, subset='log')\n",
    "dxfbval1_vol = get_lt(dxfbval1_vol_hoc, t, subset='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dwrmx',\n",
       " 'dffd',\n",
       " 'dwrzn',\n",
       " 'ddiff',\n",
       " 'hdgau',\n",
       " 'hdpt',\n",
       " 'dwrxmx',\n",
       " 'hohlca',\n",
       " 'hduni',\n",
       " 'hdod',\n",
       " 'hdzn',\n",
       " 'dc',\n",
       " 'dlogret',\n",
       " 'dohlca',\n",
       " 'dwrod',\n",
       " 'hdmx',\n",
       " 'dwrpt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([k[1] for k in f.childkeys([assets[0]])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kc_end = ['ddiff', 'ddiff_pba_vol']\n",
    "ft_all = {a: list(f.childkeys([a, *kc_end])) for a in assets}\n",
    "feat = ft_all[chosen_asset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sp_500',\n",
       "  'ddiff',\n",
       "  'ddiff_pba_vol',\n",
       "  'pba_dohlca_ddiff',\n",
       "  'pba_dohlca_ddiff(1)'],\n",
       " ['sp_500',\n",
       "  'ddiff',\n",
       "  'ddiff_pba_vol',\n",
       "  'vol_dohlca_ddiff',\n",
       "  'vol_dohlca_ddiff(1)']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = inner_join(f[feat[0]], f[feat[1]])\n",
    "# Select non-zero values and apply to_bin (converts {-1, 0, 1} series to {0, 1} by setting -1->0), my code is stupid confusing\n",
    "# Note that the label and target series are already pre-shifted from processing by xgp.py via the config files in  xg-process/\n",
    "no_zero = lambda df: df[df.values.sum(axis=1) != 0]\n",
    "to_bin = lambda df: (df+1)*.5\n",
    "feature_df, label_df, target_df = pd_common_idx_rows(features_df, to_bin(no_zero(ddir)), dret)\n",
    "assert(feature_df.shape[0]==label_df.shape[0]==target_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __setup_data__(data):\n",
    "    feature_df, label_df, target_df = data\n",
    "    ftrain, fval, ftest = map(np_at_least_nd, pd_get_np_tvt(feature_df, as_midx=False))\n",
    "    ltrain, lval, ltest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(label_df, as_midx=False))\n",
    "    ttrain, tval, ttest = map(partial(np_at_least_nd, axis=-1), pd_get_np_tvt(target_df, as_midx=False))\n",
    "    flt_train = (ftrain, ltrain, ttrain); np_assert_identical_len_dim(*flt_train)\n",
    "    flt_val = (fval, lval, tval); np_assert_identical_len_dim(*flt_val)\n",
    "    flt_test = (ftest, ltest, ttest); np_assert_identical_len_dim(*flt_test)\n",
    "\n",
    "    #self.feat_shape = ftrain.shape # Required to infer input shape of model\n",
    "    #self.num_unique_labels = max(map(lambda a: len(np.unique(a)), (ltrain, lval, ltest))) # TODO - set a variable for size of output layer automatically based on label_df\n",
    "    return flt_train, flt_val, flt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __preproc__(data, m_params):\n",
    "    return temporal_preproc(data, window_size=m_params['window_size'], apply_idx=range(len(data)))\n",
    "\n",
    "def __preproc2__(data, m_params):\n",
    "    return stride_preproc(data, window_size=m_params['window_size'])\n",
    "\n",
    "#@pl.data_loader\n",
    "def train_dataloader(t_params, flt):\n",
    "    logging.info('train_dataloader called')\n",
    "    return batchify(t_params, __preproc__(flt), False)\n",
    "\n",
    "#@pl.data_loader\n",
    "def val_dataloader(t_params, flt):\n",
    "    logging.info('val_dataloader called')\n",
    "    return batchify(t_params, __preproc__(flt), False)\n",
    "\n",
    "#@pl.data_loader\n",
    "def test_dataloader(t_params, flt):\n",
    "    logging.info('test_dataloader called')\n",
    "    return batchify(t_params, __preproc__(flt), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = __setup_data__((feature_df, label_df, target_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss': 'nll',\n",
    "    'batch_size': 1,\n",
    "    'window_size': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(params, data, shuffle_batches=False):\n",
    "\t\"\"\"\n",
    "\tReturn a torch.DataLoader made from a tuple of numpy arrays.\n",
    "\n",
    "\tArgs:\n",
    "\t\tparams (dict): model parameters dictionary\n",
    "\t\tdata (tuple): tuple of numpy arrays, features are the first element\n",
    "\t\tshuffle_batches (bool): whether or not to shuffle the batches\n",
    "\n",
    "\tReturns:\n",
    "\t\ttorch.DataLoader\n",
    "\t\"\"\"\n",
    "\tf = torch.tensor(data[0], dtype=torch.float32, requires_grad=True)\n",
    "\tif (params['loss'] in ('bce', 'bcel', 'mae', 'mse')):\n",
    "\t\tl = [torch.tensor(d, dtype=torch.float32, requires_grad=False) for d in data[1:]]\n",
    "\telif (params['loss'] in ('ce', 'nll')):\n",
    "\t\tl = [torch.tensor(d, dtype=torch.int64, requires_grad=False).squeeze() for d in data[1:]]\n",
    "\tds = TensorDataset(f, *l)\n",
    "\tdl = DataLoader(ds, batch_size=params['batch_size'], shuffle=shuffle_batches)\n",
    "\treturn ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'loss': 'nll',\n",
    "    'batch_size': 1,\n",
    "    'window_size': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping Episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1337, 10, 20) (1337, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "train_ol = __preproc__(train, params)\n",
    "val_ol = __preproc__(val, params)\n",
    "test_ol = __preproc__(test, params)\n",
    "print(train_ol[0].shape, train_ol[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Overlapping Episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67, 10, 20) (67, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "train_nol = __preproc2__(train, params)\n",
    "val_nol = __preproc2__(val, params)\n",
    "test_nol = __preproc2__(test, params)\n",
    "print(train_nol[0].shape, train_nol[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 5.0\n"
     ]
    }
   ],
   "source": [
    "tr_split = .75\n",
    "w = params['window_size']\n",
    "tr, ts = int(tr_split*w), (1-tr_split)*w\n",
    "print(tr, ts)\n",
    "\n",
    "scs = []\n",
    "for x, y, z in zip(*train_nol):\n",
    "    x, y = x.T, y\n",
    "    clf = LogisticRegression(C=10**-2, l1_ratio=.9, penalty='elasticnet', random_state=0, solver='saga').fit(x[0:tr], y[0:tr].squeeze())\n",
    "    #clf = LinearRegression().fit(x[0:tr], y[0:tr].squeeze())\n",
    "    #yh = clf.predict(x[tr:])\n",
    "    #|yp = clf.predict_proba(x[tr:])\n",
    "    sc = clf.score(x[tr:], y[tr:].squeeze())\n",
    "    scs.append(sc)\n",
    "    #print(sc)\n",
    "    #print(x)\n",
    "    #print(y.squeeze())\n",
    "    #print(z.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6417910447761197"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(scs).mean()-train_nol[1].squeeze().mean())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
