{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:script location: /home/kev/crunch/model/model_test.ipynb\n",
      "WARNING:root:using project dir: /home/kev/crunch/\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from functools import partial, reduce\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "\n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'model_test.ipynb'      # FILL\n",
    "dir_name = 'model'              # FILL\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Normal, Categorical, constraints\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pyro\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "import pyro.distributions as dist\n",
    "from dask import delayed, compute\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interact_manual, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, is_type, pd_common_idx_rows, remove_dups_list, set_loglevel, chained_filter, get_variants, dump_df, load_json, gb_transpose, np_inner, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, benchmark\n",
    "from common_util import isnt, midx_get_level, midx_intersect, str_to_list, pd_common_idx_rows, midx_split, pd_midx_to_arr, window_iter, np_is_ndim, get_class_name, get0\n",
    "from model.common import DATASET_DIR, HOPT_WORKER_BIN, default_model, default_backend, default_dataset, default_trials_count\n",
    "from model.data_util import datagen, align_first_last_cols, prune_nulls, prepare_transpose_data, prepare_label_data, prepare_target_data\n",
    "from model.model_util import BINARY_CLF_MAP\n",
    "from recon.dataset_util import prep_dataset, gen_group\n",
    "from recon.split_util import get_train_test_split, gen_time_series_split, index_three_split, pd_binary_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loglevel('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels_eod.json',\n",
       " 'mvp_dnorm_raw.json',\n",
       " 'drl.json',\n",
       " 'mvp_labels_eod.json',\n",
       " 'dnorm_raw_pba_ohlca.json',\n",
       " 'mvp_labels_fbxeod.json',\n",
       " 'mvp_dnorm_raw_pba_avgprice.json',\n",
       " 'dma.json',\n",
       " 'dnorm_raw.json',\n",
       " 'raw_pba_ohlca.json',\n",
       " 'targets_eod.json',\n",
       " 'mvp_targets_eod.json',\n",
       " 'row_masks.json',\n",
       " 'ddiff.json',\n",
       " 'dnorm_sym.json',\n",
       " 'mvp_targets_fbxeod.json',\n",
       " 'sym_raw.json',\n",
       " 'dnorm_dmx_raw_pba_ohlca.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Fixed Experiment Parameters (\"Commandline\" Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = None\n",
    "cmd_input = {\n",
    "    'model=': 'BinTCN',\n",
    "    'backend=': 'pytorch',\n",
    "    'dataset=': 'dnorm_dmx_raw_pba_ohlca.json',\n",
    "    'assets=': 'sp_500', # 'russell_2000'\n",
    "    'trials_count=': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = cmd_input['model='] if (cmd_input['model='] is not None) else default_model\n",
    "backend_name = cmd_input['backend='] if (cmd_input['backend='] is not None) else default_backend\n",
    "dataset_fname = cmd_input['dataset='] if (cmd_input['dataset='] is not None) else default_dataset\n",
    "assets = str_to_list(cmd_input['assets=']) if (cmd_input['assets='] is not None) else None\n",
    "trials_count = int(cmd_input['trials_count=']) if (cmd_input['trials_count='] is not None) else default_trials_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obj = BINARY_CLF_MAP[backend_name][model_code]()\n",
    "mod_name = get_class_name(mod_obj)\n",
    "dataset_name = dataset_fname[:-JSON_SFX_LEN]\n",
    "dataset_dict = load_json(dataset_fname, dir_path=DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': [['norm', 'dnorm_dmx_raw_pba_ohlca']],\n",
       " 'labels': 'mvp_labels_eod.json',\n",
       " 'targets': 'mvp_targets_eod.json',\n",
       " 'row_masks': 'row_masks.json'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prep_dataset(dataset_dict, assets=assets, filters_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:model: BinaryTCN\n",
      "INFO:root:backend: pytorch\n",
      "INFO:root:dataset: 1 dnorm_dmx_raw_pba_ohlca df(s)\n",
      "INFO:root:assets: sp_500\n"
     ]
    }
   ],
   "source": [
    "logging.info('model: {}'.format(mod_name))\n",
    "logging.info('backend: {}'.format(backend_name))\n",
    "logging.info('dataset: {} {} df(s)'.format(len(dataset['features']['dfs']), dataset_name))\n",
    "logging.info('assets: {}'.format(str('all' if (assets==None) else ', '.join(assets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Data Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:0 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod_direod[:], raw_pba_oc_retxeod_reteod[:]))\n",
      "INFO:root:1 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.25%)_direod[:], raw_pba_oc_retxeod(0.25%)_reteod[:]))\n",
      "INFO:root:2 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*avg,1)_direod[:], raw_pba_oc_retxeod(0.5*avg,1)_reteod[:]))\n",
      "INFO:root:3 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*max,1)_direod[:], raw_pba_oc_retxeod(0.5*max,1)_reteod[:]))\n",
      "INFO:root:4 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*min,1)_direod[:], raw_pba_oc_retxeod(0.5*min,1)_reteod[:]))\n",
      "INFO:root:5 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*std,1)_direod[:], raw_pba_oc_retxeod(0.5*std,1)_reteod[:]))\n",
      "INFO:root:6 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.50%)_direod[:], raw_pba_oc_retxeod(0.50%)_reteod[:]))\n",
      "INFO:root:7 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*avg,1)_direod[:], raw_pba_oc_retxeod(1*avg,1)_reteod[:]))\n",
      "INFO:root:8 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*max,1)_direod[:], raw_pba_oc_retxeod(1*max,1)_reteod[:]))\n",
      "INFO:root:9 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*min,1)_direod[:], raw_pba_oc_retxeod(1*min,1)_reteod[:]))\n",
      "INFO:root:10 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*std,1)_direod[:], raw_pba_oc_retxeod(1*std,1)_reteod[:]))\n",
      "INFO:root:11 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1.00%)_direod[:], raw_pba_oc_retxeod(1.00%)_reteod[:]))\n",
      "INFO:root:12 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1.50%)_direod[:], raw_pba_oc_retxeod(1.50%)_reteod[:]))\n",
      "INFO:root:13 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*avg,1)_direod[:], raw_pba_oc_retxeod(2*avg,1)_reteod[:]))\n",
      "INFO:root:14 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*max,1)_direod[:], raw_pba_oc_retxeod(2*max,1)_reteod[:]))\n",
      "INFO:root:15 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*min,1)_direod[:], raw_pba_oc_retxeod(2*min,1)_reteod[:]))\n",
      "INFO:root:16 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*std,1)_direod[:], raw_pba_oc_retxeod(2*std,1)_reteod[:]))\n",
      "INFO:root:17 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2.00%)_direod[:], raw_pba_oc_retxeod(2.00%)_reteod[:]))\n"
     ]
    }
   ],
   "source": [
    "flts_data = []\n",
    "flts_choices = {}\n",
    "for i, (fpath, lpath, tpath, frec, lrec, trec, fcol, lcol, tcol, flt) in enumerate(datagen(dataset, feat_prep_fn=prepare_transpose_data, label_prep_fn=prepare_label_data, target_prep_fn=prepare_target_data, how='df_to_df', delayed=True)):\n",
    "    ident = '{fdesc}[{fcol}], {ldesc}[{lcol}], {tdesc}[{tcol}])'.format(fdesc=frec.desc, fcol=fcol, ldesc=lrec.desc, lcol=lcol, tdesc=trec.desc, tcol=tcol)\n",
    "    logging.info('{data_idx} (X, y, z) -> ({data_id})'.format(data_idx=i, data_id=ident))\n",
    "    flts_data.append(flt)\n",
    "    flts_choices[ident] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Feature and Label/Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:735: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:708: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels = getattr(columns, 'labels', None) or [\n"
     ]
    }
   ],
   "source": [
    "feature, label, target = flts_data[1].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature\n",
    "pos_l, neg_l = pd_binary_clip(label) # Clip Label by Side\n",
    "l = pos_l\n",
    "t = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if (torch.cuda.is_available()) else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = .2\n",
    "test_ratio = .2\n",
    "train_ratio = 1-(val_ratio+test_ratio)\n",
    "f_train_idx, f_val_idx, f_test_idx = midx_split(f.index, train_ratio, val_ratio, test_ratio)\n",
    "l_train_idx, l_val_idx, l_test_idx = midx_split(l.index, train_ratio, val_ratio, test_ratio)\n",
    "t_train_idx, t_val_idx, t_test_idx = midx_split(t.index, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_pd, f_val_pd, f_test_pd = f.loc[f_train_idx], f.loc[f_val_idx], f.loc[f_test_idx]\n",
    "l_train_pd, l_val_pd, l_test_pd = l.loc[l_train_idx], l.loc[l_val_idx], l.loc[l_test_idx]\n",
    "t_train_pd, t_val_pd, t_test_pd = t.loc[t_train_idx], t.loc[t_val_idx], t.loc[t_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (is_type(f.index, pd.core.index.MultiIndex)):\n",
    "    f_train_np, f_val_np, f_test_np = map(pd_midx_to_arr, [f_train_pd.stack(), f_val_pd.stack(), f_test_pd.stack()])\n",
    "else:\n",
    "    f_train_np, f_val_np, f_test_np = f_train_pd.values, f_val_pd.values, f_test_pd.values\n",
    "l_train_np, l_val_np, l_test_np = l_train_pd.values, l_val_pd.values, l_test_pd.values\n",
    "t_train_np, t_val_np, t_test_np = t_train_pd.values, t_val_pd.values, t_test_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tar = torch.tensor(t_val_np, dtype=torch.float32, device=dev, requires_grad=False).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(f_train_np.shape[-2:]) if (len(f_train_np.shape) > 2) else (1, f_train_np.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (f_train_np, l_train_np)\n",
    "val_data = (f_val_np, l_val_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': <hyperopt.pyll.base.Apply at 0x7f2605373f28>,\n",
       " 'batch_size': <hyperopt.pyll.base.Apply at 0x7f260531c0b8>,\n",
       " 'loss': <hyperopt.pyll.base.Apply at 0x7f26987408d0>,\n",
       " 'opt': <hyperopt.pyll.base.Apply at 0x7f260912ae48>,\n",
       " 'input_windows': <hyperopt.pyll.base.Apply at 0x7f26987158d0>,\n",
       " 'topology': <hyperopt.pyll.base.Apply at 0x7f2698715ba8>,\n",
       " 'kernel_size': <hyperopt.pyll.base.Apply at 0x7f2698740278>,\n",
       " 'dropout': <hyperopt.pyll.base.Apply at 0x7f2698740470>,\n",
       " 'attention': <hyperopt.pyll.base.Apply at 0x7f2698740630>,\n",
       " 'max_attn_len': <hyperopt.pyll.base.Apply at 0x7f26987407b8>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_obj.get_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf = list(reversed(l_train_pd.value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 128, #256\n",
    "    'loss': 'nll',\n",
    "    'cw': pmf,\n",
    "    'cw': None,\n",
    "    'opt': {\n",
    "        'name': 'Adam',\n",
    "        'lr': .0001\n",
    "    },\n",
    "    'input_windows': 20,\n",
    "    'topology': [30],\n",
    "    'kernel_size': 8,\n",
    "    'dropout': 0,\n",
    "    'attention': False,\n",
    "    'max_attn_len': 80,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (f_train_np, l_train_np)\n",
    "val_data = (f_val_np, l_val_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Loss Compute Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloss(params, model, loss_function, feat_batch, lab_batch, optimizer=None, ret_train_pred=False, metrics_fns=mod_obj.metrics_fns):\n",
    "    \"\"\"\n",
    "    Compute loss and metrics on batch, run optimizer on losses if passed.\n",
    "    \"\"\"\n",
    "    # logging.debug('batch tensor[0][0]: {}'.format(feat_batch[0][0]))\n",
    "    outputs_batch = model(feat_batch)\n",
    "    loss = loss_function(outputs_batch, lab_batch)\n",
    "    max_batch, pred_batch = torch.max(outputs_batch, dim=1) # Convert network outputs into predictions\n",
    "    lab_batch_cpu = lab_batch.cpu()\n",
    "    pred_batch_cpu = pred_batch.cpu()\n",
    "    metrics = {name: fn(lab_batch_cpu, pred_batch_cpu) for name, fn in metrics_fns.items()}\n",
    "\n",
    "    if (optimizer is not None):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (not ret_train_pred):\n",
    "            return loss.item(), len(feat_batch), metrics\n",
    "\n",
    "#     logging.debug('batch loss:   {}'.format(loss.item()))\n",
    "    return loss.item(), len(feat_batch), metrics, (max_batch.exp(), pred_batch.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TCN_Classifier(\n",
       "  (tcn): TemporalConvNet(\n",
       "    (network): Sequential(\n",
       "      (0): TemporalBlock(\n",
       "        (conv1): Conv1d(5, 240, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "        (chomp1): Chomp1d()\n",
       "        (relu1): ReLU()\n",
       "        (dropout1): Dropout(p=0)\n",
       "        (conv2): Conv1d(240, 240, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "        (chomp2): Chomp1d()\n",
       "        (relu2): ReLU()\n",
       "        (dropout2): Dropout(p=0)\n",
       "        (net): Sequential(\n",
       "          (0): Conv1d(5, 240, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "          (1): Chomp1d()\n",
       "          (2): ReLU()\n",
       "          (3): Dropout(p=0)\n",
       "          (4): Conv1d(240, 240, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "          (5): Chomp1d()\n",
       "          (6): ReLU()\n",
       "          (7): Dropout(p=0)\n",
       "        )\n",
       "        (downsample): Conv1d(5, 240, kernel_size=(1,), stride=(1,))\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Linear(in_features=240, out_features=2, bias=True)\n",
       "  (logprob): LogSoftmax()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make Model, Loss Fn, and Optimizer\n",
    "clf_mdl = mod_obj.get_model(params, input_shape).to(device=dev)\n",
    "loss_fn = mod_obj.make_loss_fn(params, None if (isnt(params['cw'])) else torch.tensor(params['cw']).to(dev)).to(dev)\n",
    "opt = mod_obj.make_optimizer(params, clf_mdl.parameters())\n",
    "display(clf_mdl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Pyro Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.get_param_store().clear()\n",
    "tcn_cnn_offset = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyro_model(data, mdl, dev):\n",
    "    \"\"\"\n",
    "    Generative Model - model for how observations are generated\n",
    "    \"\"\"\n",
    "    priors = {}\n",
    "    # Go through all convolutional layers and create distribution priors\n",
    "    for i, block in enumerate(mdl.tcn.network):\n",
    "        for j in range(2):\n",
    "            prior_weight = Normal(loc=torch.zeros_like(block[j*tcn_cnn_offset].weight), scale=torch.ones_like(block[j*tcn_cnn_offset].weight))\n",
    "            prior_bias = Normal(loc=torch.zeros_like(block[j*tcn_cnn_offset].bias), scale=torch.ones_like(block[j*tcn_cnn_offset].bias))\n",
    "            priors['blk{}-{}.weight'.format(i, j)] = prior_weight\n",
    "            priors['blk{}-{}.bias'.format(i, j)] = prior_bias\n",
    "    \n",
    "    # Distribution priors of output layer\n",
    "    prior_weight = Normal(loc=torch.zeros_like(mdl.out.weight), scale=torch.ones_like(mdl.out.weight))\n",
    "    prior_bias = Normal(loc=torch.zeros_like(mdl.out.bias), scale=torch.ones_like(mdl.out.bias))\n",
    "    priors['out.weight'] = prior_weight\n",
    "    priors['out.bias'] = prior_bias\n",
    "    \n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", mdl, priors)\n",
    "\n",
    "    # sample a classifier (which also samples w and b)\n",
    "    lifted_clf = lifted_module()\n",
    "    \n",
    "    for i in pyro.plate(\"data_loop\", len(data)):\n",
    "        return pyro.sample(\"obs\", Categorical(logits=lifted_clf), obs=data[1])\n",
    "\n",
    "    for i in pyro.plate(\"data_loop\", N, subsample=data, device=dev):\n",
    "        x_data = data[:, :-1]\n",
    "        y_data = data[:, -1]\n",
    "        # run the regressor forward conditioned on inputs\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze()\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, Variable(torch.ones(data.size(0))).type_as(data)),\n",
    "                    obs=y_data.squeeze())\n",
    "        pyro.sample(\"obs\", Categorical(logits=lifted_clf), obs=data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyro_guide(data, mdl, dev):\n",
    "    \"\"\"\n",
    "    Variational Distribution - Approximation of the Posterior Distribution\n",
    "    \"\"\"\n",
    "    priors = {}\n",
    "    for i, block in enumerate(mdl.tcn.network):\n",
    "        for j in range(2):\n",
    "            weight_mu_param = pyro.param('blk{}-{}_weight_mu'.format(i, j), torch.randn_like(block[j*tcn_cnn_offset].weight))\n",
    "            weight_sigma_param = softplus(pyro.param('blk{}-{}_weight_sigma'.format(i, j), torch.randn_like(block[j*tcn_cnn_offset].weight)))\n",
    "            weight_prior = Normal(loc=weight_mu_param, scale=weight_sigma_param)\n",
    "            bias_mu_param = pyro.param('blk{}-{}_bias_mu'.format(i, j), torch.randn_like(block[j*tcn_cnn_offset].bias))\n",
    "            bias_sigma_param = softplus(pyro.param('blk{}-{}_bias_sigma'.format(i, j), torch.randn_like(block[j*tcn_cnn_offset].bias)))\n",
    "            bias_prior = Normal(loc=bias_mu_param, scale=bias_sigma_param)\n",
    "            priors['blk{}-{}.weight'.format(i, j)] = weight_prior\n",
    "            priors['blk{}-{}.bias'.format(i, j)] = bias_prior\n",
    "\n",
    "    weight_mu_param = pyro.param('out_weight_mu', torch.randn_like(net.out.weight))\n",
    "    weight_sigma_param = softplus(pyro.param('out_weight_sigma', torch.randn_like(net.out.weight)))\n",
    "    weight_prior = Normal(loc=weight_mu_param, scale=weight_sigma_param)\n",
    "    bias_mu_param = pyro.param('out_bias_mu', torch.randn_like(net.out.bias))\n",
    "    bias_sigma_param = softplus(pyro.param('out_bias_sigma', torch.randn_like(net.out.bias)))\n",
    "    bias_prior = Normal(loc=bias_mu_param, scale=bias_sigma_param)\n",
    "    priors['out.weight'] = weight_prior\n",
    "    priors['out.bias'] = bias_prior\n",
    "\n",
    "    lifted_module = pyro.random_module(\"module\", mdl, priors)\n",
    "\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro_optim = Adam({\"lr\": 0.001})\n",
    "svi = SVI(pyro_model, pyro_guide, pyro_optim, loss=Trace_ELBO())\n",
    "svi_history = {\n",
    "    'loss': [],\n",
    "    'val_loss': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c347ea665598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmod_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msvi_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-c347ea665598>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mXb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmod_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmod_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msvi_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36m_loss_and_grads\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss_and_grads\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0m_loss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                     \u001b[0mloss_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "for epoch in range(params['epochs']):\n",
    "    epoch_str = str(epoch).zfill(3)\n",
    "    losses, nums = zip(*[(svi.step((Xb, yb), clf_mdl), len(Xb)) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, train_data), dev, shuffle_batches=True)])\n",
    "    loss = np_inner(losses, nums)\n",
    "    svi_history['loss'].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro_optim = Adam({\"lr\": 0.001})\n",
    "svi = SVI(pyro_model, pyro_guide, pyro_optim, loss=\"ELBO\")\n",
    "\n",
    "N = len(X_train)\n",
    "\n",
    "for j in range(3000):\n",
    "    epoch_loss = 0.0\n",
    "    perm = torch.randperm(N)\n",
    "    # shuffle data\n",
    "    data = data[perm]\n",
    "    # get indices of each batch\n",
    "    all_batches = get_batch_indices(N, 64)\n",
    "    for ix, batch_start in enumerate(all_batches[:-1]):\n",
    "        batch_end = all_batches[ix + 1]\n",
    "        batch_data = data[batch_start: batch_end]        \n",
    "        epoch_loss += svi.step(batch_data)\n",
    "    if j % 100 == 0:\n",
    "        print(j, \"avg loss {}\".format(epoch_loss/float(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = self.tbx(params, logdir) if (logdir is not None) else None\n",
    "mdl.zero_grad()\n",
    "opt.zero_grad()\n",
    "\n",
    "# Metrics\n",
    "history = {\n",
    "    'loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "for name in mod_obj.metrics_fns.keys():\n",
    "    history[name] = []\n",
    "    history['val_{}'.format(name)] = []\n",
    "\n",
    "# Fit Model\n",
    "try:\n",
    "    for epoch in range(params['epochs']):\n",
    "        epoch_str = str(epoch).zfill(3)\n",
    "\n",
    "        mdl.train()\n",
    "        losses, nums, metrics = zip(*[bloss(params, mdl, loss_fn, Xb, yb, optimizer=opt) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, train_data), dev, shuffle_batches=True)])\n",
    "        loss = np_inner(losses, nums)\n",
    "        soa = {name[0]: tuple(d[name[0]] for d in metrics) for name in zip(*metrics)}\n",
    "        metric = {name: np_inner(vals, nums) for name, vals in soa.items()}\n",
    "        history['loss'].append(loss)\n",
    "        for name, val in metric.items():\n",
    "            history[name].append(val)\n",
    "\n",
    "        mdl.eval()\n",
    "        with torch.no_grad():\n",
    "            Xe, ye = get0(*mod_obj.batchify(params, mod_obj.preproc(params, val_data), dev, override_batch_size=val_data[-1].size, shuffle_batches=False))\n",
    "            loss, num, metric, pred = bloss(params, mdl, loss_fn, Xe, ye)\n",
    "            pred_conf, pred_dir = pred\n",
    "        history['val_loss'].append(loss)\n",
    "        for name, val in metric.items():\n",
    "            history['val_{}'.format(name)].append(val)\n",
    "\n",
    "    results = {\n",
    "#         'history': history,\n",
    "        'mean': {name: np.mean(vals) for name, vals in history.items()},\n",
    "        'last': {name: vals[-1] for name, vals in history.items()}\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error('Error during model fitting: {}'.format(str(e)))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = l_train_pd.value_counts(normalize=True, sort=True, ascending=True).values\n",
    "val_range = l_val_pd.value_counts(normalize=True, sort=True, ascending=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      #0         #1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train [0.46604215 0.53395785]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val [0.47188755 0.52811245]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('      #0         #1')\n",
    "display('train {}'.format(train_range))\n",
    "display('val {}'.format(val_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.0029391172548962965,\n",
       " 'val_loss': 2.336294651031494,\n",
       " 'acc': 1.0,\n",
       " 'val_acc': 0.5015353121801432}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = val_tar[val_tar.size()[0]-pred_dir.size()[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3285, device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3652, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_conf * pred_dir) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5334, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
