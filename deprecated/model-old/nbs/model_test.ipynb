{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:script location: /home/kev/crunch/model/model_test.ipynb\n",
      "WARNING:root:using project dir: /home/kev/crunch/\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from functools import partial, reduce\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "\n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'model_test.ipynb'      # FILL\n",
    "dir_name = 'model'              # FILL\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from dask import delayed, compute\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interact_manual, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, is_type, pd_common_idx_rows, remove_dups_list, set_loglevel, chained_filter, get_variants, dump_df, load_json, gb_transpose, np_inner, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, benchmark\n",
    "from common_util import midx_get_level, midx_intersect, str_to_list, pd_common_idx_rows, midx_split, pd_midx_to_arr, window_iter, np_is_ndim, get_class_name\n",
    "from model.common import DATASET_DIR, HOPT_WORKER_BIN, default_model, default_backend, default_dataset, default_trials_count\n",
    "from model.data_util import datagen, align_first_last_cols, prune_nulls, prepare_transpose_data, prepare_label_data, prepare_target_data\n",
    "from model.model_util import BINARY_CLF_MAP\n",
    "from recon.dataset_util import prep_dataset, gen_group\n",
    "from recon.split_util import get_train_test_split, gen_time_series_split, index_three_split, pd_binary_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loglevel('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Fixed Experiment Parameters (\"Commandline\" Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = None\n",
    "cmd_input = {\n",
    "    'model=': 'BinTCN',\n",
    "    'backend=': 'pytorch',\n",
    "    'dataset=': 'raw_pba_ohlca.json', # 'dnorm_raw_pba_ohlca.json'\n",
    "    'assets=': 'sp_500', # 'russell_2000'\n",
    "    'trials_count=': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = cmd_input['model='] if (cmd_input['model='] is not None) else default_model\n",
    "backend_name = cmd_input['backend='] if (cmd_input['backend='] is not None) else default_backend\n",
    "dataset_fname = cmd_input['dataset='] if (cmd_input['dataset='] is not None) else default_dataset\n",
    "assets = str_to_list(cmd_input['assets=']) if (cmd_input['assets='] is not None) else None\n",
    "trials_count = int(cmd_input['trials_count=']) if (cmd_input['trials_count='] is not None) else default_trials_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obj = BINARY_CLF_MAP[backend_name][model_code]()\n",
    "mod_name = get_class_name(mod_obj)\n",
    "dataset_name = dataset_fname[:-JSON_SFX_LEN]\n",
    "dataset_dict = load_json(dataset_fname, dir_path=DATASET_DIR)\n",
    "dataset = prep_dataset(dataset_dict, assets=assets, filters_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:model: BinaryTCN\n",
      "INFO:root:backend: pytorch\n",
      "INFO:root:dataset: 1 raw_pba_ohlca df(s)\n",
      "INFO:root:assets: sp_500\n"
     ]
    }
   ],
   "source": [
    "logging.info('model: {}'.format(mod_name))\n",
    "logging.info('backend: {}'.format(backend_name))\n",
    "logging.info('dataset: {} {} df(s)'.format(len(dataset['features']['dfs']), dataset_name))\n",
    "logging.info('assets: {}'.format(str('all' if (assets==None) else ', '.join(assets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Data Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:0 (X, y, z) -> (raw[:], raw_pba_oc_retxeod_direod[:], raw_pba_oc_retxeod_reteod[:]))\n",
      "INFO:root:1 (X, y, z) -> (raw[:], raw_pba_oc_retxeod(0.25%)_direod[:], raw_pba_oc_retxeod(0.25%)_reteod[:]))\n",
      "INFO:root:2 (X, y, z) -> (raw[:], raw_pba_oc_retxeod(0.50%)_direod[:], raw_pba_oc_retxeod(0.50%)_reteod[:]))\n",
      "INFO:root:3 (X, y, z) -> (raw[:], raw_pba_oc_retxeod(1.00%)_direod[:], raw_pba_oc_retxeod(1.00%)_reteod[:]))\n",
      "INFO:root:4 (X, y, z) -> (raw[:], raw_pba_oc_retxeod(1.50%)_direod[:], raw_pba_oc_retxeod(1.50%)_reteod[:]))\n",
      "INFO:root:5 (X, y, z) -> (raw[:], raw_pba_oc_retxeod(2.00%)_direod[:], raw_pba_oc_retxeod(2.00%)_reteod[:]))\n"
     ]
    }
   ],
   "source": [
    "flts_data = []\n",
    "flts_choices = {}\n",
    "for i, (fpath, lpath, tpath, frec, lrec, trec, fcol, lcol, tcol, flt) in enumerate(datagen(dataset, feat_prep_fn=prepare_transpose_data, label_prep_fn=prepare_label_data, target_prep_fn=prepare_target_data, how='df_to_df', delayed=True)):\n",
    "    ident = '{fdesc}[{fcol}], {ldesc}[{lcol}], {tdesc}[{tcol}])'.format(fdesc=frec.desc, fcol=fcol, ldesc=lrec.desc, lcol=lcol, tdesc=trec.desc, tcol=tcol)\n",
    "    logging.info('{data_idx} (X, y, z) -> ({data_id})'.format(data_idx=i, data_id=ident))\n",
    "    flts_data.append(flt)\n",
    "    flts_choices[ident] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Feature and Label/Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:708: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels = getattr(columns, 'labels', None) or [\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:735: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n"
     ]
    }
   ],
   "source": [
    "feature, label, target = flts_data[0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature\n",
    "pos_l, neg_l = pd_binary_clip(label) # Clip Label by Side\n",
    "l = pos_l\n",
    "t = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = .2\n",
    "test_ratio = .2\n",
    "train_ratio = 1-(val_ratio+test_ratio)\n",
    "f_train_idx, f_val_idx, f_test_idx = midx_split(f.index, train_ratio, val_ratio, test_ratio)\n",
    "l_train_idx, l_val_idx, l_test_idx = midx_split(l.index, train_ratio, val_ratio, test_ratio)\n",
    "t_train_idx, t_val_idx, t_test_idx = midx_split(t.index, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_pd, f_val_pd, f_test_pd = f.loc[f_train_idx], f.loc[f_val_idx], f.loc[f_test_idx]\n",
    "l_train_pd, l_val_pd, l_test_pd = l.loc[l_train_idx], l.loc[l_val_idx], l.loc[l_test_idx]\n",
    "t_train_pd, t_val_pd, t_test_pd = t.loc[t_train_idx], t.loc[t_val_idx], t.loc[t_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (is_type(f.index, pd.core.index.MultiIndex)):\n",
    "    f_train_np, f_val_np, f_test_np = map(pd_midx_to_arr, [f_train_pd.stack(), f_val_pd.stack(), f_test_pd.stack()])\n",
    "else:\n",
    "    f_train_np, f_val_np, f_test_np = f_train_pd.values, f_val_pd.values, f_test_pd.values\n",
    "l_train_np, l_val_np, l_test_np = l_train_pd.values, l_val_pd.values, l_test_pd.values\n",
    "t_train_np, t_val_np, t_test_np = t_train_pd.values, t_val_pd.values, t_test_pd.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(f_train_np.shape[-2:]) if (len(f_train_np.shape) > 2) else (1, f_train_np.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': <hyperopt.pyll.base.Apply at 0x7f8f2d336438>,\n",
       " 'batch_size': <hyperopt.pyll.base.Apply at 0x7f8f2d336588>,\n",
       " 'loss': <hyperopt.pyll.base.Apply at 0x7f8eb1c05c88>,\n",
       " 'opt': <hyperopt.pyll.base.Apply at 0x7f8f2d3362b0>,\n",
       " 'input_windows': <hyperopt.pyll.base.Apply at 0x7f8f2d33a630>,\n",
       " 'topology': <hyperopt.pyll.base.Apply at 0x7f8f2d33a208>,\n",
       " 'kernel_size': <hyperopt.pyll.base.Apply at 0x7f8f2d33aa20>,\n",
       " 'dropout': <hyperopt.pyll.base.Apply at 0x7f8f2d33ac50>,\n",
       " 'attention': <hyperopt.pyll.base.Apply at 0x7f8f2d33add8>,\n",
       " 'max_attn_len': <hyperopt.pyll.base.Apply at 0x7f8f2d33ae80>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_obj.get_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 256,\n",
    "    'loss': 'ce',\n",
    "    'opt': {\n",
    "        'name': 'Adam',\n",
    "        'lr': .001\n",
    "    },\n",
    "    'input_windows': 5,\n",
    "    'topology': [5, 3, 1],\n",
    "    'kernel_size': 4,\n",
    "    'dropout': .2,\n",
    "    'attention': False,\n",
    "    'max_attn_len': 80,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if (torch.cuda.is_available()) else torch.device('cpu')\n",
    "mdl = mod_obj.get_model(params, input_shape).to(device=dev)\n",
    "device = dev\n",
    "model = mdl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn, opt = mod_obj.make_loss_fn(params).to(device), mod_obj.make_optimizer(params, model.parameters())\n",
    "writer = self.tbx(params, logdir) if (logdir is not None) else None\n",
    "model.zero_grad()\n",
    "opt.zero_grad()\n",
    "\n",
    "logging.debug('w[-2:][-2:]: {}'.format(list(model.parameters())[-2:][-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (f_train_np, l_train_np)\n",
    "val_data = (f_val_np, l_val_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'loss': [],\n",
    "    'val_loss': []\n",
    "}\n",
    "for name in mod_obj.metrics_fns.keys():\n",
    "    history[name] = []\n",
    "    history['val_{}'.format(name)] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Batch Loss Compute Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloss(params, model, loss_function, feat_batch, lab_batch, optimizer=None, metrics_fns=mod_obj.metrics_fns):\n",
    "    \"\"\"\n",
    "    Compute loss and metrics on batch, run optimizer on losses if passed.\n",
    "    \"\"\"\n",
    "    # logging.debug('batch tensor[0][0]: {}'.format(feat_batch[0][0]))\n",
    "    outputs_batch = model(feat_batch)\n",
    "    loss = loss_function(outputs_batch, lab_batch)\n",
    "    max_batch, pred_batch = torch.max(outputs_batch, dim=1) # Convert network outputs into predictions\n",
    "    lab_batch_cpu = lab_batch.cpu()\n",
    "    pred_batch_cpu = pred_batch.cpu()\n",
    "    metrics = {name: fn(lab_batch_cpu, pred_batch_cpu) for name, fn in metrics_fns.items()}\n",
    "\n",
    "    if (optimizer is not None):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    logging.debug('batch loss:   {}'.format(loss.item()))\n",
    "    return loss.item(), len(feat_batch), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(params['epochs']):\n",
    "        epoch_str = str(epoch).zfill(3)\n",
    "        model.train()\n",
    "        losses, nums, metrics = zip(*[bloss(params, model, loss_fn, Xb, yb, optimizer=opt) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, train_data), device, shuffle_batches=True)])\n",
    "        loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        soa = {name[0]: tuple(d[name[0]] for d in metrics) for name in zip(*metrics)}\n",
    "        metric = {name: np.sum(np.multiply(vals, nums)) / np.sum(nums) for name, vals in soa.items()}\n",
    "        logging.debug('{} train loss: {}'.format(epoch_str, loss))\n",
    "        history['loss'].append(loss)\n",
    "        history['acc'].append(metric['acc'])\n",
    "        logging.debug('{} w[-2:][-2:]: {}'.format(epoch_str, list(model.parameters())[-2:][-2:]))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums, metrics = zip(*[bloss(params, model, loss_fn, Xb, yb) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, val_data), device, shuffle_batches=False)])\n",
    "        loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
    "        soa = {name[0]: tuple(d[name[0]] for d in metrics) for name in zip(*metrics)}\n",
    "        metric = {name: np.sum(np.multiply(vals, nums)) / np.sum(nums) for name, vals in soa.items()}\n",
    "        logging.debug('{} val loss: {}'.format(epoch_str, loss))\n",
    "        history['val_loss'].append(loss)\n",
    "        history['val_acc'].append(metric['acc'])\n",
    "        logging.debug('{} w[-2:][-2:]: {}'.format(epoch_str, list(model.parameters())[-2:][-2:]))\n",
    "\n",
    "    results = {\n",
    "        # 'history': history\n",
    "        'mean': {\n",
    "            'loss': np.mean(history['loss']),\n",
    "            'val_loss': np.mean(history['val_loss']),\n",
    "            'acc': np.mean(history['acc']),\n",
    "            'val_acc': np.mean(history['val_acc'])\n",
    "        },\n",
    "        'last': {\n",
    "            'loss': history['loss'][-1],\n",
    "            'val_loss': history['val_loss'][-1],\n",
    "            'acc': history['acc'][-1],\n",
    "            'val_acc': history['val_acc'][-1]\n",
    "        }\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error('Error during model fitting: {}'.format(str(e)))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': {'loss': 3.0495534735315006,\n",
       "  'val_loss': 1.8445394916111426,\n",
       "  'acc': 0.5011155778894473,\n",
       "  'val_acc': 0.5046068548387097},\n",
       " 'last': {'loss': 1.4121486174201645,\n",
       "  'val_loss': 1.2944406309435446,\n",
       "  'acc': 0.5018425460636516,\n",
       "  'val_acc': 0.4465725806451613}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(params['epochs']):\n",
    "        epoch_str = str(epoch).zfill(3)\n",
    "\n",
    "        model.train()\n",
    "        losses, nums, metrics = zip(*[bloss(params, model, loss_fn, Xb, yb, optimizer=opt) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, train_data), device, shuffle_batches=True)])\n",
    "        loss = np_inner(losses, nums)\n",
    "        soa = {name[0]: tuple(d[name[0]] for d in metrics) for name in zip(*metrics)}\n",
    "        metric = {name: np_inner(vals, nums) for name, vals in soa.items()}\n",
    "        history['loss'].append(loss)\n",
    "        for name, val in metric.items():\n",
    "            history[name].append(val)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            losses, nums, metrics = zip(*[bloss(params, model, loss_fn, Xb, yb) for Xb, yb in mod_obj.batchify(params, mod_obj.preproc(params, val_data), device, shuffle_batches=False)])\n",
    "        loss = np_inner(losses, nums)\n",
    "        soa = {name[0]: tuple(d[name[0]] for d in metrics) for name in zip(*metrics)}\n",
    "        metric = {name: np_inner(vals, nums) for name, vals in soa.items()}\n",
    "        logging.debug('{} val loss: {}'.format(epoch_str, loss))\n",
    "        history['val_loss'].append(loss)\n",
    "        for name, val in metric.items():\n",
    "            history['val_{}'.format(name)].append(val)\n",
    "\n",
    "    results = {\n",
    "        'history': history,\n",
    "        'mean': {name: np.mean(vals) for name, vals in history.items()},\n",
    "        'last': {name: vals[-1] for name, vals in history.items()}\n",
    "    }\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error('Error during model fitting: {}'.format(str(e)))\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.9973970999752655,\n",
       " 'val_loss': 1.4383501185044165,\n",
       " 'acc': 0.501318257956449,\n",
       " 'val_acc': 0.4990221774193549,\n",
       " 'f1': 0.38219632835718587,\n",
       " 'val_f1': 0.3129545060022222}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 1.2067930588570472,\n",
       " 'val_loss': 0.9496929222537626,\n",
       " 'acc': 0.5055276381909548,\n",
       " 'val_acc': 0.4465725806451613,\n",
       " 'f1': 0.3592858176627701,\n",
       " 'val_f1': 0.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
