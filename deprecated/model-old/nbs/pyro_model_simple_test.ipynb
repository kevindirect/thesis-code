{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:script location: /home/kev/crunch/model/model_test.ipynb\n",
      "WARNING:root:using project dir: /home/kev/crunch/\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from os import sep\n",
    "from os.path import dirname, realpath\n",
    "from pathlib import Path\n",
    "from functools import partial, reduce\n",
    "import logging\n",
    "\n",
    "def get_cwd(fname, subdir, crunch_dir=realpath(Path.home()) +sep +'crunch' +sep):\n",
    "    \"\"\"\n",
    "    Convenience function to make a directory string for the current file based on inputs.\n",
    "    Jupyter Notebook in Anaconda invokes the Python interpreter in Anaconda's subdirectory\n",
    "    which is why changing sys.argv[0] is necessary. In the future a better way to do this\n",
    "    should be preferred..\n",
    "    \"\"\"\n",
    "    return crunch_dir +subdir +fname\n",
    "\n",
    "def fix_path(cwd):\n",
    "    \"\"\"\n",
    "    Convenience function to fix argv and python path so that jupyter notebook can run the same as\n",
    "    any script in crunch.\n",
    "    \"\"\"\n",
    "    sys.argv[0] = cwd\n",
    "    module_path = os.path.abspath(os.path.join('..'))\n",
    "    if module_path not in sys.path:\n",
    "        sys.path.append(module_path)\n",
    "\n",
    "fname = 'model_test.ipynb'      # FILL\n",
    "dir_name = 'model'              # FILL\n",
    "fix_path(get_cwd(fname, dir_name +sep))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import torch\n",
    "# from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.distributions import constraints\n",
    "import pyro\n",
    "from pyro.contrib.autoguide import AutoMultivariateNormal\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.distributions import Normal, Categorical, Bernoulli\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(37)\n",
    "from dask import delayed, compute\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interact_manual, interactive, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "from common_util import RECON_DIR, JSON_SFX_LEN, DT_CAL_DAILY_FREQ, is_type, pd_common_idx_rows, remove_dups_list, set_loglevel, chained_filter, get_variants, dump_df, load_json, gb_transpose, np_inner, pd_common_index_rows, filter_cols_below, inner_join, outer_join, ser_shift, list_get_dict, window_iter, pyt_unsqueeze_to, pyt_reverse_dim_order, benchmark\n",
    "from common_util import isnt, midx_get_level, midx_intersect, str_to_list, pd_common_idx_rows, midx_split, pd_midx_to_arr, window_iter, np_is_ndim, get_class_name, get0\n",
    "from model.common import DATASET_DIR, HOPT_WORKER_BIN, INTRADAY_LEN, default_model, default_backend, default_dataset, default_trials_count\n",
    "from model.data_util import datagen, align_first_last_cols, prune_nulls, prepare_transpose_data, prepare_label_data, prepare_target_data\n",
    "from model.model_util import BINARY_CLF_MAP\n",
    "from recon.dataset_util import prep_dataset, gen_group\n",
    "from recon.split_util import get_train_test_split, gen_time_series_split, index_three_split, pd_binary_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_loglevel('info')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labels_eod.json',\n",
       " 'mvp_dnorm_raw.json',\n",
       " 'drl.json',\n",
       " 'mvp_labels_eod.json',\n",
       " 'dnorm_raw_pba_ohlca.json',\n",
       " 'mvp_labels_fbxeod.json',\n",
       " 'mvp_dnorm_raw_pba_avgprice.json',\n",
       " 'dma.json',\n",
       " 'dnorm_raw.json',\n",
       " 'raw_pba_ohlca.json',\n",
       " 'targets_eod.json',\n",
       " 'mvp_targets_eod.json',\n",
       " 'row_masks.json',\n",
       " 'ddiff.json',\n",
       " 'dnorm_sym.json',\n",
       " 'mvp_targets_fbxeod.json',\n",
       " 'sym_raw.json',\n",
       " 'dnorm_dmx_raw_pba_ohlca.json']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Fixed Experiment Parameters (\"Commandline\" Arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = None\n",
    "cmd_input = {\n",
    "    'model=': 'BinTCN',\n",
    "    'backend=': 'pytorch',\n",
    "    'dataset=': 'dnorm_dmx_raw_pba_ohlca.json',\n",
    "    'assets=': 'sp_500', # 'russell_2000'\n",
    "    'trials_count=': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_code = cmd_input['model='] if (cmd_input['model='] is not None) else default_model\n",
    "backend_name = cmd_input['backend='] if (cmd_input['backend='] is not None) else default_backend\n",
    "dataset_fname = cmd_input['dataset='] if (cmd_input['dataset='] is not None) else default_dataset\n",
    "assets = str_to_list(cmd_input['assets=']) if (cmd_input['assets='] is not None) else None\n",
    "trials_count = int(cmd_input['trials_count=']) if (cmd_input['trials_count='] is not None) else default_trials_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_obj = BINARY_CLF_MAP[backend_name][model_code]()\n",
    "mod_name = get_class_name(mod_obj)\n",
    "dataset_name = dataset_fname[:-JSON_SFX_LEN]\n",
    "dataset_dict = load_json(dataset_fname, dir_path=DATASET_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features': [['norm', 'dnorm_dmx_raw_pba_ohlca']],\n",
       " 'labels': 'mvp_labels_eod.json',\n",
       " 'targets': 'mvp_targets_eod.json',\n",
       " 'row_masks': 'row_masks.json'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = prep_dataset(dataset_dict, assets=assets, filters_map=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:model: BinaryTCN\n",
      "INFO:root:backend: pytorch\n",
      "INFO:root:dataset: 1 dnorm_dmx_raw_pba_ohlca df(s)\n",
      "INFO:root:assets: sp_500\n"
     ]
    }
   ],
   "source": [
    "logging.info('model: {}'.format(mod_name))\n",
    "logging.info('backend: {}'.format(backend_name))\n",
    "logging.info('dataset: {} {} df(s)'.format(len(dataset['features']['dfs']), dataset_name))\n",
    "logging.info('assets: {}'.format(str('all' if (assets==None) else ', '.join(assets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Data Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:0 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod_direod[:], raw_pba_oc_retxeod_reteod[:]))\n",
      "INFO:root:1 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.25%)_direod[:], raw_pba_oc_retxeod(0.25%)_reteod[:]))\n",
      "INFO:root:2 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*avg,1)_direod[:], raw_pba_oc_retxeod(0.5*avg,1)_reteod[:]))\n",
      "INFO:root:3 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*max,1)_direod[:], raw_pba_oc_retxeod(0.5*max,1)_reteod[:]))\n",
      "INFO:root:4 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*min,1)_direod[:], raw_pba_oc_retxeod(0.5*min,1)_reteod[:]))\n",
      "INFO:root:5 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.5*std,1)_direod[:], raw_pba_oc_retxeod(0.5*std,1)_reteod[:]))\n",
      "INFO:root:6 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(0.50%)_direod[:], raw_pba_oc_retxeod(0.50%)_reteod[:]))\n",
      "INFO:root:7 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*avg,1)_direod[:], raw_pba_oc_retxeod(1*avg,1)_reteod[:]))\n",
      "INFO:root:8 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*max,1)_direod[:], raw_pba_oc_retxeod(1*max,1)_reteod[:]))\n",
      "INFO:root:9 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*min,1)_direod[:], raw_pba_oc_retxeod(1*min,1)_reteod[:]))\n",
      "INFO:root:10 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1*std,1)_direod[:], raw_pba_oc_retxeod(1*std,1)_reteod[:]))\n",
      "INFO:root:11 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1.00%)_direod[:], raw_pba_oc_retxeod(1.00%)_reteod[:]))\n",
      "INFO:root:12 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(1.50%)_direod[:], raw_pba_oc_retxeod(1.50%)_reteod[:]))\n",
      "INFO:root:13 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*avg,1)_direod[:], raw_pba_oc_retxeod(2*avg,1)_reteod[:]))\n",
      "INFO:root:14 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*max,1)_direod[:], raw_pba_oc_retxeod(2*max,1)_reteod[:]))\n",
      "INFO:root:15 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*min,1)_direod[:], raw_pba_oc_retxeod(2*min,1)_reteod[:]))\n",
      "INFO:root:16 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2*std,1)_direod[:], raw_pba_oc_retxeod(2*std,1)_reteod[:]))\n",
      "INFO:root:17 (X, y, z) -> (raw_pba_dmx[:], raw_pba_oc_retxeod(2.00%)_direod[:], raw_pba_oc_retxeod(2.00%)_reteod[:]))\n"
     ]
    }
   ],
   "source": [
    "flts_data = []\n",
    "flts_choices = {}\n",
    "for i, (fpath, lpath, tpath, frec, lrec, trec, fcol, lcol, tcol, flt) in enumerate(datagen(dataset, feat_prep_fn=prepare_transpose_data, label_prep_fn=prepare_label_data, target_prep_fn=prepare_target_data, how='df_to_df', delayed=True)):\n",
    "    ident = '{fdesc}[{fcol}], {ldesc}[{lcol}], {tdesc}[{tcol}])'.format(fdesc=frec.desc, fcol=fcol, ldesc=lrec.desc, lcol=lcol, tdesc=trec.desc, tcol=tcol)\n",
    "    logging.info('{data_idx} (X, y, z) -> ({data_id})'.format(data_idx=i, data_id=ident))\n",
    "    flts_data.append(flt)\n",
    "    flts_choices[ident] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Feature and Label/Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:708: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels = getattr(columns, 'labels', None) or [\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:735: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n",
      "  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n",
      "/home/kev/miniconda3/lib/python3.6/site-packages/pyarrow/pandas_compat.py:752: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n",
      "  labels, = index.labels\n"
     ]
    }
   ],
   "source": [
    "feature, label, target = flts_data[1].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature\n",
    "pos_l, neg_l = pd_binary_clip(label) # Clip Label by Side\n",
    "l = pos_l\n",
    "t = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if (torch.cuda.is_available()) else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into Ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = .2\n",
    "test_ratio = .2\n",
    "train_ratio = 1-(val_ratio+test_ratio)\n",
    "f_train_idx, f_val_idx, f_test_idx = midx_split(f.index, train_ratio, val_ratio, test_ratio)\n",
    "l_train_idx, l_val_idx, l_test_idx = midx_split(l.index, train_ratio, val_ratio, test_ratio)\n",
    "t_train_idx, t_val_idx, t_test_idx = midx_split(t.index, train_ratio, val_ratio, test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_pd, f_val_pd, f_test_pd = f.loc[f_train_idx], f.loc[f_val_idx], f.loc[f_test_idx]\n",
    "l_train_pd, l_val_pd, l_test_pd = l.loc[l_train_idx], l.loc[l_val_idx], l.loc[l_test_idx]\n",
    "t_train_pd, t_val_pd, t_test_pd = t.loc[t_train_idx], t.loc[t_val_idx], t.loc[t_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (is_type(f.index, pd.core.index.MultiIndex)):\n",
    "    f_train_np, f_val_np, f_test_np = map(pd_midx_to_arr, [f_train_pd.stack(), f_val_pd.stack(), f_test_pd.stack()])\n",
    "else:\n",
    "    f_train_np, f_val_np, f_test_np = f_train_pd.values, f_val_pd.values, f_test_pd.values\n",
    "l_train_np, l_val_np, l_test_np = l_train_pd.values, l_val_pd.values, l_test_pd.values\n",
    "t_train_np, t_val_np, t_test_np = t_train_pd.values, t_val_pd.values, t_test_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tar = torch.tensor(t_val_np, dtype=torch.float32, device=dev, requires_grad=False).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Input Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = tuple(f_train_np.shape[-2:]) if (len(f_train_np.shape) > 2) else (1, f_train_np.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (f_train_np, l_train_np)\n",
    "val_data = (f_val_np, l_val_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': <hyperopt.pyll.base.Apply at 0x7fc48842fc50>,\n",
       " 'batch_size': <hyperopt.pyll.base.Apply at 0x7fc3f48bb940>,\n",
       " 'loss': <hyperopt.pyll.base.Apply at 0x7fc48842f828>,\n",
       " 'opt': <hyperopt.pyll.base.Apply at 0x7fc48842fac8>,\n",
       " 'input_windows': <hyperopt.pyll.base.Apply at 0x7fc48842bb00>,\n",
       " 'topology': <hyperopt.pyll.base.Apply at 0x7fc48842b4a8>,\n",
       " 'kernel_size': <hyperopt.pyll.base.Apply at 0x7fc48842f358>,\n",
       " 'dropout': <hyperopt.pyll.base.Apply at 0x7fc48842f438>,\n",
       " 'attention': <hyperopt.pyll.base.Apply at 0x7fc48842f588>,\n",
       " 'max_attn_len': <hyperopt.pyll.base.Apply at 0x7fc48842f710>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_obj.get_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf = list(reversed(l_train_pd.value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'epochs': 100,\n",
    "    'batch_size': 128, #256\n",
    "    'loss': 'nll',\n",
    "    'cw': pmf,\n",
    "    'cw': None,\n",
    "    'opt': {\n",
    "        'name': 'Adam',\n",
    "        'lr': .0001\n",
    "    },\n",
    "    'input_windows': 5,\n",
    "    'topology': [10],\n",
    "    'kernel_size': 8,\n",
    "    'dropout': 0,\n",
    "    'attention': False,\n",
    "    'max_attn_len': 80,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Preprocessing and Batchificaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(params, data):\n",
    "    \"\"\"\n",
    "    Reshaping transform for temporal data.\n",
    "\n",
    "    Runs a \"moving window unstack\" operation through the first data such that each row of the result contains the history\n",
    "    of the original up to and including that row based on a input_windows parameter in params. The input_windows\n",
    "    determines how far back the history each row will record; a input_windows of '1' results in no change.\n",
    "    This method also adds a singleton dimension between the first and second after the moving window unstack; this is to\n",
    "    denote the \"number of channels\" for CNN based learning algorithms.\n",
    "\n",
    "    example with input_windows of '2':\n",
    "                                                0 | a b c \n",
    "                                                1 | d e f ---> 1 | a b c d e f\n",
    "                                                2 | g h i      2 | d e f g h i\n",
    "                                                3 | j k l      3 | g h i j k l\n",
    "\n",
    "    All data after the first tuple item are assumed to be label/target vectors and are reshaped to align with the new first\n",
    "    tuple item.\n",
    "    \"\"\"\n",
    "    # Reshape features into overlapping moving window samples\n",
    "    f = np.array([np.concatenate(vec, axis=-1) for vec in window_iter(data[0], n=params['input_windows'])])\n",
    "\n",
    "    if (len(f.shape) < 3):\n",
    "        f = np.expand_dims(f, 1) # Add a singleton dimension for single channel data if needed\n",
    "\n",
    "    l = []\n",
    "    for vec in data[1:]:\n",
    "        r = vec[params['input_windows']-1:]\t\t\t\t\t\t\t\t# Realign by dropping lables prior to the first step\n",
    "        m = np.expand_dims(r, 1) if (np_is_ndim(vec)) else r \t\t\t# Make array vector of vectors if it is one dimensional\n",
    "        l.append(m)\n",
    "\n",
    "    return (f, *l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(params, data, device, override_batch_size=None, shuffle_batches=False):\n",
    "    \"\"\"\n",
    "    Takes in final numpy data and returns torch DataLoader over torch tensor minibatches of specified torch device.\n",
    "    \"\"\"\n",
    "    f = torch.tensor(data[0], dtype=torch.float32, device=device, requires_grad=True)\n",
    "    if (params['loss'] in ['bce', 'bcel']):\n",
    "        l = [torch.tensor(d, dtype=torch.float32, device=device, requires_grad=False) for d in data[1:]]\n",
    "    elif (params['loss'] in ['ce', 'nll']):\n",
    "        l = [torch.tensor(d, dtype=torch.float32, device=device, requires_grad=False).squeeze() for d in data[1:]]\n",
    "    ds = TensorDataset(f, *l)\n",
    "    dl = DataLoader(ds, batch_size=params['batch_size'] if (isnt(override_batch_size)) else override_batch_size, shuffle=shuffle_batches)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (f_train_np, l_train_np)\n",
    "val_data = (f_val_np, l_val_np)\n",
    "train_dl = batchify(params, preproc(params, train_data), dev, shuffle_batches=True)\n",
    "val_dl = batchify(params, preproc(params, val_data), dev, override_batch_size=val_data[-1].size, shuffle_batches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xb: torch.Size([128, 5, 40]), yb: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "for Xb, yb in train_dl:\n",
    "    print('Xb: {}, yb: {}'.format(Xb.shape, yb.shape))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.h1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_embedding = self.h1(x)\n",
    "        out_score = self.relu(self.out(out_embedding))\n",
    "        out_prob = self.log_softmax(out_score)\n",
    "        return out_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 200 400 1\n"
     ]
    }
   ],
   "source": [
    "chunk = train_data[0].shape[1] * train_data[0].shape[2]\n",
    "channel_chunk = chunk\n",
    "batch_size = params['batch_size']\n",
    "input_size = chunk * params['input_windows']\n",
    "hidden_size = chunk * params['topology'][0]\n",
    "output_size = 1\n",
    "print(batch_size, input_size, hidden_size, output_size)\n",
    "net = Net(input_size, hidden_size, output_size).to(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 200])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0598, 0.0000, 0.4629,\n",
      "        0.0000, 0.0849, 0.0000, 0.2883, 0.1847, 0.1470, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.2478, 0.0000, 0.4654, 0.0000, 0.0000, 0.0000, 0.0123, 0.0000,\n",
      "        0.1761, 0.0000, 0.0000, 0.0000, 0.0792, 0.0074, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0420, 0.0730, 0.0000, 0.0000, 0.2345, 0.0000, 0.2320,\n",
      "        0.0000, 0.2375, 0.1283, 0.0000, 0.2735, 0.0000, 0.2961, 0.0000, 0.0000,\n",
      "        0.0000, 0.1338, 0.0119, 0.0000, 0.0000, 0.0813, 0.0205, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0944, 0.1692, 0.1305, 0.0000, 0.0501,\n",
      "        0.0000, 0.4059, 0.0000, 0.0760, 0.0000, 0.2544, 0.0000, 0.1592, 0.0530,\n",
      "        0.0231, 0.1915, 0.0000, 0.1715, 0.0000, 0.3514, 0.0000, 0.0000, 0.1123,\n",
      "        0.0000, 0.2179, 0.0875, 0.0000, 0.1536, 0.0000, 0.1482, 0.2847, 0.0000,\n",
      "        0.6408, 0.0000, 0.2461, 0.1544, 0.2377, 0.0000, 0.0000, 0.2728, 0.0000,\n",
      "        0.2316, 0.5361, 0.1879, 0.0000, 0.2230, 0.0317, 0.0000, 0.0000, 0.2582,\n",
      "        0.0000, 0.0000, 0.0000, 0.0141, 0.1618, 0.0000, 0.0000, 0.0000, 0.2695,\n",
      "        0.0000, 0.3823], device='cuda:0', grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for j in range(params['epochs']):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_id, data_batch in enumerate(train_dl):\n",
    "        data_batch[0] = data_batch[0].reshape(-1, input_size)\n",
    "#         data_batch[0] = pyt_reverse_dim_order(data_batch[0])\n",
    "        print(data_batch[0].shape)\n",
    "        print(net(data_batch[0]).squeeze())\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (h1): Linear(in_features=200, out_features=400, bias=True)\n",
       "  (out): Linear(in_features=400, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (log_softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Generative Model\n",
    "Samples an embedding model using priors $P(A)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "softplus = torch.nn.Softplus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "    h1w_prior = Normal(loc=torch.zeros_like(net.h1.weight, device=dev), scale=torch.ones_like(net.h1.weight, device=dev))\n",
    "    h1b_prior = Normal(loc=torch.zeros_like(net.h1.bias, device=dev), scale=torch.ones_like(net.h1.bias, device=dev))\n",
    "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight, device=dev), scale=torch.ones_like(net.out.weight, device=dev))\n",
    "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias, device=dev), scale=torch.ones_like(net.out.bias, device=dev))\n",
    "\n",
    "    priors = {\n",
    "                'h1.weight': h1w_prior.independent(2),\n",
    "                'h1.bias': h1b_prior.independent(1),\n",
    "                'out.weight': outw_prior.independent(2),\n",
    "                'out.bias': outb_prior.independent(1)\n",
    "    }\n",
    "    lifted_module = pyro.random_module('module', net, priors)\n",
    "    lifted_model = lifted_module().to(dev)\n",
    "\n",
    "#     with pyro.plate('plate', batch_size, device=dev):\n",
    "#         pyro.sample('obs', Bernoulli(logits=lifted_model(data[0]).squeeze()).independent(1), obs=data[1])\n",
    "        \n",
    "    with pyro.plate('plate', batch_size, device=dev):\n",
    "        pyro.sample('obs', Categorical(logits=lifted_model(data[0])).independent(1), obs=data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Variational Inference Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    # Hidden layer weight and bias distribution priors\n",
    "    h1w_mu_param = pyro.param('h1w_mu', torch.randn_like(net.h1.weight, device=dev))\n",
    "    h1w_sigma_param = softplus(pyro.param('h1w_sigma', torch.randn_like(net.h1.weight, device=dev)))\n",
    "    h1b_mu_param = pyro.param('h1b_mu', torch.randn_like(net.h1.bias, device=dev))\n",
    "    h1b_sigma_param = softplus(pyro.param('h1b_sigma', torch.randn_like(net.h1.bias, device=dev)))\n",
    "    h1w_prior = Normal(loc=h1w_mu_param, scale=h1w_sigma_param)\n",
    "    h1b_prior = Normal(loc=h1b_mu_param, scale=h1b_sigma_param)\n",
    "\n",
    "    # Output layer weight and bias distribution priors\n",
    "    outw_mu_param = pyro.param('outw_mu', torch.randn_like(net.out.weight, device=dev))\n",
    "    outw_sigma_param = softplus(pyro.param('outw_sigma', torch.randn_like(net.out.weight, device=dev)))\n",
    "    outb_mu_param = pyro.param('outb_mu', torch.randn_like(net.out.bias, device=dev))\n",
    "    outb_sigma_param = softplus(pyro.param('outb_sigma', torch.randn_like(net.out.bias, device=dev)))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param)\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "\n",
    "    priors = {\n",
    "                'h1.weight': h1w_prior.independent(2),\n",
    "                'h1.bias': h1b_prior.independent(1),\n",
    "                'out.weight': outw_prior.independent(2),\n",
    "                'out.bias': outb_prior.independent(1)\n",
    "    }\n",
    "\n",
    "    lifted_module = pyro.random_module('module', net, priors)\n",
    "    return lifted_module().to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guide = AutoMultivariateNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SVI Optimization Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.001})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error while computing log_prob at site 'obs':\nThe value argument must be within the support\n           Trace Shapes:              \n            Param Sites:              \n           Sample Sites:              \n module$$$h1.weight dist     | 400 200\n                   value     | 400 200\n                log_prob     |        \n   module$$$h1.bias dist     | 400    \n                   value     | 400    \n                log_prob     |        \nmodule$$$out.weight dist     |   1 400\n                   value     |   1 400\n                log_prob     |        \n  module$$$out.bias dist     |   1    \n                   value     |   1    \n                log_prob     |        \n                obs dist 128 | 128    \n                   value     | 128    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/independent.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sum_rightmost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinterpreted_batch_ndims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument must be within the support",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-13890c78e76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         print(data_batch[1].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# calculate the loss and take a gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 52\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mmodel_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_subsample_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0mguide_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_score_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    221\u001b[0m                                     ValueError(\"Error while computing log_prob at site '{}':\\n{}\\n{}\"\n\u001b[1;32m    222\u001b[0m                                                .format(name, exc_value, shapes)),\n\u001b[0;32m--> 223\u001b[0;31m                                     traceback)\n\u001b[0m\u001b[1;32m    224\u001b[0m                     \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unscaled_log_prob\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_and_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/pyro/poutine/trace_struct.py\u001b[0m in \u001b[0;36mcompute_log_prob\u001b[0;34m(self, site_filter)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"log_prob\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                         \u001b[0mlog_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kwargs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/independent.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sum_rightmost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinterpreted_batch_ndims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m_validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The value argument must be within the support'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_checked_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error while computing log_prob at site 'obs':\nThe value argument must be within the support\n           Trace Shapes:              \n            Param Sites:              \n           Sample Sites:              \n module$$$h1.weight dist     | 400 200\n                   value     | 400 200\n                log_prob     |        \n   module$$$h1.bias dist     | 400    \n                   value     | 400    \n                log_prob     |        \nmodule$$$out.weight dist     |   1 400\n                   value     |   1 400\n                log_prob     |        \n  module$$$out.bias dist     |   1    \n                   value     |   1    \n                log_prob     |        \n                obs dist 128 | 128    \n                   value     | 128    "
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "for j in range(iterations):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_id, data_batch in enumerate(train_dl):\n",
    "        data_batch[0] = data_batch[0].reshape(-1, input_size)\n",
    "#         data_batch[0] = pyt_reverse_dim_order(data_batch[0])\n",
    "#         data_batch[1] = pyt_unsqueeze_to(data_batch[1], data_batch[0].dim())\n",
    "#         data_batch[1] = pyt_reverse_dim_order(data_batch[1])\n",
    "#         print(data_batch[0].shape)\n",
    "#         print(data_batch[1].shape)\n",
    "        # calculate the loss and take a gradient step\n",
    "        epoch_loss += svi.step(data_batch) / len(data_batch)\n",
    "    \n",
    "    if (j % 100 == 0):\n",
    "        print(\"Epoch {} loss: {} \".format(j, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "def predict(x):\n",
    "    sampled_models = [guide(None) for _ in range(num_samples)]\n",
    "    yhats = [model(x).data for model in sampled_models]\n",
    "    mean = torch.mean(torch.stack(yhats), 0)\n",
    "    return np.argmax(mean.cpu().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction when network is forced to predict\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eq() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-7cc01c0f82b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy: %d %%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: eq() received an invalid combination of arguments - got (numpy.ndarray), but expected one of:\n * (Tensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n * (Number other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mnumpy.ndarray\u001b[0m)\n"
     ]
    }
   ],
   "source": [
    "print('Prediction when network is forced to predict')\n",
    "correct = 0\n",
    "total = 0\n",
    "for j, data_batch in enumerate(val_dl):\n",
    "    data_batch[0] = data_batch[0].reshape(-1, input_size)\n",
    "    predicted = predict(data_batch[0])\n",
    "    total += data_batch[1].size(0)\n",
    "    correct += (predicted == data_batch[1]).sum().item()\n",
    "print(\"accuracy: %d %%\" % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in range(100):\n",
    "    for data_batch in val_dl:\n",
    "        data_batch[0] = data_batch[0].reshape(-1, input_size)\n",
    "        sampled_model = guide(data_batch[0])\n",
    "        pred = sampled_model(data_batch[0]).data.cpu().numpy().flatten()\n",
    "        preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('h1w_mu', array([[ 0.06061935,  0.78333384,  0.9648807 , ..., -0.49211693,\n",
      "        -0.39323905, -1.3315772 ],\n",
      "       [-1.8267834 , -0.58437175, -0.30872568, ...,  0.262584  ,\n",
      "        -1.2409006 ,  0.607822  ],\n",
      "       [ 1.800545  , -0.30165407,  1.1286403 , ...,  0.5021332 ,\n",
      "         0.22980645,  0.49681905],\n",
      "       ...,\n",
      "       [-0.25249648, -1.8088641 , -0.08010844, ...,  2.4268255 ,\n",
      "         0.35542777, -0.23396581],\n",
      "       [ 2.1971266 , -0.93359876,  1.3790072 , ...,  0.73735726,\n",
      "        -2.3852098 , -1.2463719 ],\n",
      "       [-1.0418869 , -2.4348843 , -0.8035702 , ..., -1.7562288 ,\n",
      "        -1.2917346 , -0.9114405 ]], dtype=float32))\n",
      "('h1w_sigma', array([[-0.40544316, -2.3487933 , -0.93532467, ..., -0.28864723,\n",
      "        -0.9305421 ,  0.1803772 ],\n",
      "       [ 0.4386679 ,  1.2234645 ,  0.7307406 , ...,  1.6464808 ,\n",
      "        -0.49710417,  0.9930913 ],\n",
      "       [ 0.44662485,  0.22739658,  0.9912317 , ..., -0.3423796 ,\n",
      "         0.9007679 , -0.35563305],\n",
      "       ...,\n",
      "       [ 0.04292707,  1.6384434 ,  0.33961257, ...,  0.54104376,\n",
      "         0.04814048,  0.16796733],\n",
      "       [ 0.62522966,  0.26605678,  0.21312386, ...,  0.49654028,\n",
      "        -0.7873607 ,  0.85675865],\n",
      "       [-0.15968755,  1.3401108 ,  0.19989437, ...,  0.83743376,\n",
      "         0.5757322 , -1.8978771 ]], dtype=float32))\n",
      "('h1b_mu', array([-0.5369538 ,  0.6498566 , -0.7636205 , -0.35586113,  1.598667  ,\n",
      "       -0.10417153, -0.7061833 ,  1.1211337 , -0.6898324 , -1.3022896 ,\n",
      "        2.0499752 ,  1.6345308 ,  0.53199685,  0.5705864 ,  1.0187932 ,\n",
      "       -0.20732921, -2.5141673 , -0.87878186, -2.2271276 ,  1.4169313 ,\n",
      "       -0.15839228, -1.4700125 ,  0.7301026 , -0.5958957 , -0.7888885 ,\n",
      "        0.00296663, -2.3348951 , -0.19895972,  0.15939015,  1.4026277 ,\n",
      "       -0.5548789 ,  0.4523821 ,  0.23143193, -0.611871  ,  0.98971295,\n",
      "        0.22463995,  0.14228787, -1.0235336 , -0.40998107,  1.1398898 ,\n",
      "        1.0870917 , -0.59901   , -1.2813915 ,  1.5754505 ,  0.36781743,\n",
      "        0.38670933, -1.7341906 , -1.3952742 , -1.2461258 , -0.8218318 ,\n",
      "       -0.31191602, -2.905104  , -1.6748306 ,  0.5283194 ,  1.7522397 ,\n",
      "       -0.48395023, -0.41311207,  0.60845745, -0.16927908, -1.8644298 ,\n",
      "       -1.7001288 , -0.27292898, -0.7283142 ,  0.6931401 ,  0.7523305 ,\n",
      "        2.1171546 , -0.42726105, -0.40886903, -0.9237602 , -0.29088476,\n",
      "       -0.5876026 ,  0.14176016, -0.15304251, -0.7522634 ,  1.1512691 ,\n",
      "       -0.78991425, -0.13165727, -0.5631315 , -0.958232  ,  0.88826567],\n",
      "      dtype=float32))\n",
      "('h1b_sigma', array([ 0.44691762, -0.47814   , -0.26711535,  0.29758906,  1.4958532 ,\n",
      "        0.2845045 , -0.22713405, -0.7518518 , -2.0069592 ,  1.0405542 ,\n",
      "       -1.5625435 , -0.16730525, -0.8172756 , -1.349515  , -0.2592186 ,\n",
      "       -0.6304439 ,  0.94386077, -0.86359334, -0.10430446, -0.4167576 ,\n",
      "        1.2385824 , -0.778483  ,  0.13155027, -0.08777339, -0.2790375 ,\n",
      "       -1.5150715 ,  0.05479148,  0.31392166,  0.7000591 ,  0.05988005,\n",
      "       -1.1357942 , -1.6299423 , -0.07875443, -2.6069627 ,  0.03940362,\n",
      "       -0.00348117, -0.01528124, -1.0544499 , -0.7618515 ,  1.1562368 ,\n",
      "        0.8400108 ,  0.17851251,  0.20922424, -0.97712463, -0.48566586,\n",
      "       -1.2740388 ,  0.5472634 , -0.47275338, -1.4651116 ,  0.20952801,\n",
      "        0.31063902,  0.49327043, -0.84404063, -0.1675319 , -1.2566171 ,\n",
      "        0.29419783,  0.4296588 , -0.7720998 ,  0.69352716, -1.4592288 ,\n",
      "        0.9428334 ,  0.80675054, -1.1426271 , -2.3293374 ,  0.9722747 ,\n",
      "       -1.41443   ,  0.35554695, -0.9114612 ,  0.24419093, -0.27986056,\n",
      "        0.26927352,  0.94498116, -1.3615013 , -0.660018  , -0.5671659 ,\n",
      "        0.10412233, -0.47437286, -0.41859138, -0.43235227, -0.29936886],\n",
      "      dtype=float32))\n",
      "('outw_mu', array([[ 0.27888313,  1.0316116 , -1.070083  ,  1.3316576 , -0.24079435,\n",
      "         0.9843545 ,  1.5032119 , -0.93395996,  0.06314287,  1.9554365 ,\n",
      "         0.22212668, -0.02627826,  0.48164067, -0.52552754, -0.6454775 ,\n",
      "         1.4748021 , -0.2200644 , -1.8675903 ,  1.272438  , -0.5166871 ,\n",
      "        -1.9729656 ,  1.2768822 ,  1.3286933 ,  1.1268693 , -0.17153399,\n",
      "        -0.5924483 ,  0.09054776,  1.9150624 ,  1.4814277 ,  1.4923489 ,\n",
      "         0.7581516 ,  0.6448411 , -0.09761122, -1.0437906 ,  1.6132638 ,\n",
      "         0.06323202, -0.89228964,  1.1750102 , -0.35573772, -1.4211452 ,\n",
      "        -0.237269  , -0.01233558,  1.113225  , -0.3780655 , -2.9828017 ,\n",
      "         0.1121942 ,  0.72276706,  0.31217906,  0.7618429 ,  1.6440369 ,\n",
      "         0.5809591 ,  0.14153707,  0.53807765, -0.5368183 , -0.55571854,\n",
      "        -1.2968565 ,  1.9464233 ,  1.9313613 , -0.70017767,  1.4059265 ,\n",
      "         0.01962469,  2.4681444 , -2.3245127 , -0.15659808,  1.480665  ,\n",
      "        -1.8776984 , -0.00864138, -0.5201652 , -0.5009275 ,  1.897097  ,\n",
      "         0.39281237,  0.09503296,  1.2853501 , -0.9188803 ,  0.23846865,\n",
      "         1.2323791 ,  0.60553586, -0.9505651 ,  0.36233172,  0.6080586 ]],\n",
      "      dtype=float32))\n",
      "('outw_sigma', array([[ 0.6245103 , -0.8854328 ,  0.8948471 ,  1.098525  , -0.04272436,\n",
      "         1.0959241 ,  0.3958656 ,  0.1721197 , -1.5343033 , -0.583643  ,\n",
      "         2.9493184 ,  0.98839396,  1.2222774 ,  0.8976108 ,  0.23561029,\n",
      "         0.54531264,  0.4459352 , -2.0038288 ,  0.48813313,  0.50502264,\n",
      "         1.2263367 ,  1.2038969 ,  0.54933304,  1.1889495 , -1.1903588 ,\n",
      "         0.43357167,  0.33256394,  1.103892  , -0.30120197, -0.756039  ,\n",
      "        -1.8632321 , -0.27655077,  0.2117297 , -0.05585922, -1.957969  ,\n",
      "         1.0165762 ,  0.38044766, -0.02531587,  0.3160571 , -1.6587894 ,\n",
      "        -1.5007538 ,  0.34615412,  0.02073429, -0.85578144,  0.5133597 ,\n",
      "         0.7356427 , -0.43926227,  0.405274  ,  0.52146363,  0.99312115,\n",
      "         0.54864573,  1.510079  , -0.70175105, -0.42184663, -1.2491864 ,\n",
      "        -0.80521035,  0.5345529 ,  1.2566131 , -0.9610602 ,  0.3141921 ,\n",
      "         0.37571928, -0.46904078,  1.245523  , -0.8181974 ,  1.2447318 ,\n",
      "         0.37980458,  0.03197086, -0.75640464,  0.35990453,  0.20985499,\n",
      "         0.7417662 , -0.20064603, -0.09673614, -1.2531685 , -0.22459975,\n",
      "        -0.29705748, -1.2598898 ,  1.2970345 , -0.7997593 ,  0.5425304 ]],\n",
      "      dtype=float32))\n",
      "('outb_mu', array([0.47575155], dtype=float32))\n",
      "('outb_sigma', array([0.4675198], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "for name in pyro.get_param_store().get_all_param_names():\n",
    "    print((name, pyro.param(name).data.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = l_train_pd.value_counts(normalize=True, sort=True, ascending=True).values\n",
    "val_range = l_val_pd.value_counts(normalize=True, sort=True, ascending=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      #0         #1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'train [0.46604215 0.53395785]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'val [0.47188755 0.52811245]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display('      #0         #1')\n",
    "display('train {}'.format(train_range))\n",
    "display('val {}'.format(val_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 0.0029391172548962965,\n",
       " 'val_loss': 2.336294651031494,\n",
       " 'acc': 1.0,\n",
       " 'val_acc': 0.5015353121801432}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt = val_tar[val_tar.size()[0]-pred_dir.size()[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3285, device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dir @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3652, device='cuda:0')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_conf * pred_dir) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5334, device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model(data):\n",
    "#     w_tensor_shape, b_tensor_shape = (hidden_size, input_size), (hidden_size, )\n",
    "#     h1w_prior = Normal(loc=torch.zeros(w_tensor_shape, device=dev), scale=torch.ones(w_tensor_shape, device=dev))\n",
    "#     h1b_prior = Normal(loc=torch.zeros(b_tensor_shape, device=dev), scale=torch.ones(b_tensor_shape, device=dev))\n",
    "#     w_tensor_shape, b_tensor_shape = (output_size, hidden_size), (output_size, )\n",
    "#     outw_prior = Normal(loc=torch.zeros(w_tensor_shape, device=dev), scale=torch.ones(w_tensor_shape, device=dev))\n",
    "#     outb_prior = Normal(loc=torch.zeros(b_tensor_shape, device=dev), scale=torch.ones(b_tensor_shape, device=dev))\n",
    "\n",
    "#     h1w_prior = Normal(loc=torch.zeros_like(net.h1.weight, device=dev), scale=torch.ones_like(net.h1.weight, device=dev))\n",
    "#     h1b_prior = Normal(loc=torch.zeros_like(net.h1.bias, device=dev), scale=torch.ones_like(net.h1.bias, device=dev))\n",
    "#     outw_prior = Normal(loc=torch.zeros_like(net.out.weight, device=dev), scale=torch.ones_like(net.out.weight, device=dev))\n",
    "#     outb_prior = Normal(loc=torch.zeros_like(net.out.bias, device=dev), scale=torch.ones_like(net.out.bias, device=dev))\n",
    "\n",
    "#     priors = {\n",
    "#                 'h1.weight': h1w_prior,\n",
    "#                 'h1.bias': h1b_prior,\n",
    "#                 'out.weight': outw_prior,\n",
    "#                 'out.bias': outb_prior\n",
    "#     }\n",
    "#     print('model prior shape - e: {}, b: {}'.format(priors['h1.weight'].event_shape,  priors['h1.weight'].batch_shape))\n",
    "    \n",
    "#     lifted_module = pyro.random_module('module', net, priors)\n",
    "#     lifted_model = lifted_module().to(dev)\n",
    "\n",
    "#     with pyro.plate('plate', size=batch_size, device=dev) as smp:\n",
    "#         pyro.sample('obs', Categorical(logits=lifted_model(data[0])), obs=data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guide(data):\n",
    "    dist_batch_shape = (data[0].shape[-1],)\n",
    "\n",
    "    # First layer weight distribution priors\n",
    "    h1w_mu_param = pyro.param('h1w_mu', torch.randn_like(net.h1.weight, device=dev))\n",
    "    h1w_sigma_param = softplus(pyro.param('h1w_sigma', torch.randn_like(net.h1.weight, device=dev)))\n",
    "    h1w_prior = Normal(loc=h1w_mu_param, scale=h1w_sigma_param)\n",
    "\n",
    "    # First layer bias distribution priors\n",
    "    h1b_mu_param = pyro.param('h1b_mu', torch.randn_like(net.h1.bias, device=dev))\n",
    "    h1b_sigma_param = softplus(pyro.param('h1b_sigma', torch.randn_like(net.h1.bias, device=dev)))\n",
    "    h1b_prior = Normal(loc=h1b_mu_param, scale=h1b_sigma_param)\n",
    "\n",
    "    # Output layer weight distribution priors\n",
    "    outw_mu_param = pyro.param('outw_mu', torch.randn_like(net.out.weight, device=dev))\n",
    "    outw_sigma_param = softplus(pyro.param('outw_sigma', torch.randn_like(net.out.weight, device=dev)))\n",
    "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param)\n",
    "\n",
    "    # Output layer bias distribution priors\n",
    "    outb_mu_param = pyro.param('outb_mu', torch.randn_like(net.out.bias, device=dev))\n",
    "    outb_sigma_param = softplus(pyro.param('outb_sigma', torch.randn_like(net.out.bias, device=dev)))\n",
    "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
    "\n",
    "    priors = {\n",
    "                'h1.weight': h1w_prior.to_event(event_dims),\n",
    "                'h1.bias': h1b_prior.to_event(event_dims),\n",
    "                'out.weight': outw_prior.to_event(event_dims),\n",
    "                'out.bias': outb_prior.to_event(event_dims)\n",
    "    }\n",
    "    print('guide prior shape - e: {}, b: {}'.format(priors['h1.weight'].event_shape,  priors['h1.weight'].batch_shape))\n",
    "\n",
    "    lifted_module = pyro.random_module('module', net, priors)\n",
    "    return lifted_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
