# Kevin Patel

import sys
from os import sep
from os.path import isfile, getsize
import logging

import psycopg2
import pandas as pd
from pandas.util import hash_pandas_object

from common_util import DATA_DIR, load_df, dump_df, makedir_if_not_exists, quote_it, search_df, str_now, benchmark
from data.common import DATA_SCHEMA_DIR, db_schema_name, db_init_file, DB_INFO


class SQL_Data_Backend:
	"""
	PostgreSQL Backend for data api.
	"""

	def __init__(self):
		self.con = None
		self.cur = None
		conn_terms = ["{}='{}'".format(key, val) for key, val in DB_INFO.items()]

		try:
			self.con = psycopg2.connect(" ".join(conn_terms))
			self.cur = con.cursor()
			if (not self.schema_exists()):
				self.init_schema()

		except psycopg2.DatabaseError as e:
			if (self.con):
				self.con.rollback()
		 
			logging.error('Error %s' % e)
			sys.exit(1)

	def print_tables(self):
		self.cur.execute("SELECT tablename FROM pg_tables WHERE schemaname='{}';".format('public'))

		for table in cur.fetchall():
			columns = []
			self.cur.execute("SELECT * FROM {};".format(table[0]))
			rows = cur.fetchall()
			print('columns:', columns)
			print('\n'.join([str(row) for row in rows]))
		print()

	def schema_exists(self, schema_name=db_schema_name):
		sql_str = "SELECT schema_name FROM information_schema.schemata WHERE schema_name='{}';"
		self.cur.execute(sql_str.format(schema_name))
		result = cur.fetchone()
		return (result is not None and result[0]==actual)

	def init_schema(self):
		init_path = DATA_SCHEMA_DIR +db_init_file
		self.cur.execute(open(init_path, "r").read())

	def clear_schema(self):
		self.cur.execute("DROP SCHEMA {} CASCADE;".format(db_schema_name))

	def reset_schema(self):
		self.clear_schema()
		self.init_schema()

	


	@classmethod
	def assert_valid_entry(cls, entry):
		"""Assert whether or not entry is a validly formatted entry to the data record."""
		# XXX - convert to set operations, easier to read and understand

		# Required fields:
		assert all((col in entry and entry[col] is not None) for col in DR_REQ), 'missing a required general rec entry field (DR_REQ)'
		assert all((col in entry and entry[col] is not None) for col in DR_STAGE if col.startswith(entry['stage'])), 'missing a required stage rec entry field (DR_STAGE)'

		# Required omissions (autogenerated):
		assert all((col not in entry) for col in DR_IDS), 'can\'t pass an autogenerated rec entry field (DR_IDS)'
		assert all((col not in entry) for col in DR_GEN), 'can\'t pass an autogenerated rec entry field (DR_GEN)'

	@classmethod
	def get_id(cls, entry):
		"""Return id of matched entry in the df record, else return a new id."""
		match = search_df(cls.DATA_RECORD, entry)
		entry_id = len(cls.DATA_RECORD.index) if (match.empty) else match.values[0]
		return entry_id, match.empty

	@classmethod
	def get_name(cls, entry):
		return '_'.join([entry['root'], entry['stage'], str(entry['id'])])

	@classmethod
	def ss_to_path_loader(cls, entry):
		"""
		Return stage specific field to string subdirectory mapping function.
		Inner function accesses entry fields via closure.
		"""
		def ss_to_path(col_name):
			field = entry[col_name]
			return {
				list: (lambda: '_'.join(field)),		  # Preserves field ordering in path
				set: (lambda: '_'.join(sorted(field))),
				str: (lambda: field),
				None: (lambda: str(field))
			}.get(type(field), None)()

		return ss_to_path

	@classmethod
	def get_path(cls, entry):
		"""Return path of df on disk for given candidate entry"""
		path_dir = DATA_DIR +sep.join([entry['root'], entry['basis'], entry['freq']]) +sep

		sorted_ss = sorted(filter(lambda c: c.startswith(entry['stage']), DR_STAGE))
		if (sorted_ss):
			path_dir += sep.join(map(cls.ss_to_path_loader(entry), sorted_ss)) +sep

		return path_dir

	@classmethod
	def matched(cls, search_dict):
		"""Yield iterator of NamedTuples from matched entry subset"""
		match_ids = search_df(cls.DATA_RECORD, search_dict)
		yield from cls.DATA_RECORD.loc[match_ids].itertuples()

	@classmethod
	def loader(cls, **kwargs):
		"""Return a loader function that takes a record entry and returns something"""

		def load_rec_df(rec):
			return rec, load_df(rec.name, dir_path=rec.dir, dti_freq=rec.freq, **kwargs)
		
		return load_rec_df

	@classmethod
	def dump(cls, df, entry, update_record=False):
		"""
		XXX - break this down and make it more elegant
		"""
		entry['id'], is_new = cls.get_id(entry)
		entry['name'] = cls.get_name(entry)
		entry['dir'] = cls.get_path(entry)

		makedir_if_not_exists(entry['dir'])
		with benchmark('', suppress=True) as b:
			entry['size'] = dump_df(df, entry['name'], dir_path=entry['dir'])
		entry['dumptime'] = round(b.time, 2)
		entry['hash'] = sum(hash_pandas_object(df))
		addition = pd.DataFrame(columns=DR_COLS, index=[entry['id']])

		if (is_new):
			entry['created'] = str_now()
			addition.loc[entry['id']] = entry
			cls.DATA_RECORD = pd.concat([cls.DATA_RECORD, addition], copy=False)
		else:
			entry['modified'] = str_now()
			addition.loc[entry['id']] = entry
			cls.DATA_RECORD.update(addition)

		if (update_record):
			cls.dump_record()
