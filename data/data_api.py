"""
Kevin Patel
"""
import sys
from os import sep
from os.path import isfile, getsize
from copy import deepcopy
from collections import defaultdict
import logging

from dask import delayed
import pandas as pd
from pandas.util import hash_pandas_object

from common_util import DATA_DIR, load_df, dump_df, NestedDefaultDict, makedir_if_not_exists, chained_filter, search_df, query_df, recursive_dict, list_get_dict, list_set_dict, dict_path, isnt,  str_now, benchmark
from data.common import DR_FNAME, DR_FMT, DR_COLS, DR_IDS, DR_CAT, DR_DIR, DR_INFO, DR_GEN, DR_NAME
from data.axe import axe_get, axe_process, axe_join, axe_get_keychain


class DataAPI:
	"""
	The Global API used to load or dump dataframes. All real implementation is done in the inner class, the outer
	class is just a generic interface. This is to make swapping out the backend easier
	XXX:
		- move DataRecordAPI to it's own class file, have DataAPI inherit from it
		- implement a SQL backend for DataRecordAPI
	"""
	class DataRecordAPI:
		"""
		Global storage structure for the storage of dataframe records dumped by other stages.
		Currently only supports single threaded access.

		Entry:
			id (int): integer id
			name (str): filename of df on disk
			root (str): the root dependency of this df; for raw stage data this is a join group but for others it's a df name
			basis (str): the direct dependency (parent) of this dataframe; for raw stage data root always equals basis
			stage (str): the stage the data originated from
		"""
		@classmethod
		def reload_record(cls):
			cls.DATA_RECORD = load_df(DR_FNAME, dir_path=DATA_DIR, data_format=DR_FMT)
			assert list(cls.DATA_RECORD.columns)==DR_COLS, 'loaded data record columns don\'t match schema'

		@classmethod
		def reset_record(cls):
			cls.DATA_RECORD = pd.DataFrame(columns=DR_COLS)

		@classmethod
		def dump_record(cls):
			dump_df(cls.DATA_RECORD, DR_FNAME, dir_path=DATA_DIR, data_format=DR_FMT)

		@classmethod
		def get_record_view(cls):
			return cls.DATA_RECORD.loc[:, :]

		@classmethod
		def assert_valid_entry(cls, entry):
			"""Assert whether or not entry is a validly formatted entry to the data record."""
			# XXX - deprecated -> delete, no longer necessary make_entry does better validation
			# XXX - convert to set operations, easier to read and understand

			# Required fields:
			assert all((col in entry and entry[col] is not None) for col in DR_REQ), 'missing a required general rec entry field (DR_REQ)'

			# Required omissions (autogenerated):
			assert all((col not in entry) for col in DR_IDS), 'can\'t pass an autogenerated rec entry field (DR_IDS)'
			assert all((col not in entry) for col in DR_GEN), 'can\'t pass an autogenerated rec entry field (DR_GEN)'

		@classmethod
		def get_id(cls, entry):
			"""
			Return id of matched entry in the df record, else return a new id.
			"""
			match = search_df(cls.DATA_RECORD, entry)
			entry_id = len(cls.DATA_RECORD.index) if (match.empty) else match.values[0]
			return entry_id, match.empty

		@classmethod
		def get_name(cls, entry):
			"""
			Return a unique name field (used as filename of df on disk)
			"""
			return '_'.join([str(entry[field]) for field in DR_NAME])

		@classmethod
		def get_path(cls, entry):
			"""
			Return path suffix of df on disk for given candidate entry
			"""
			return sep.join([entry[field] for field in DR_DIR]) +sep

		@classmethod
		def ss_to_path_loader(cls, entry):
			"""
			XXX - deprecated -> delete
			Return stage specific field to string subdirectory mapping function.
			Inner function accesses entry fields via closure.
			"""
			def ss_to_path(col_name):
				field = entry[col_name]
				return {
					list: (lambda: '_'.join(field)),		  # Preserves field ordering in path
					set: (lambda: '_'.join(sorted(field))),
					str: (lambda: field),
					None: (lambda: str(field))
				}.get(type(field), None)()

			return ss_to_path

		@classmethod
		def matched(cls, search_dict, direct_query=False):
			"""
			Yield from iterator of NamedTuples from matched entry subset
			"""
			if (direct_query):
				match_ids = query_df(cls.DATA_RECORD, search_dict)
			else:
				match_ids = search_df(cls.DATA_RECORD, search_dict)
			yield from cls.DATA_RECORD.loc[match_ids].itertuples()

		@classmethod
		def loader(cls, **kwargs):
			"""Return a loader function that takes a record entry and returns something"""

			def load_rec_df(rec):
				return rec, load_df(rec.name, dir_path=DATA_DIR+rec.dir, dti_freq=rec.freq, **kwargs)

			return load_rec_df

		@classmethod
		def dump(cls, df, entry, path_pfx=DATA_DIR, update_record=False):
			"""
			XXX - break this down and make it more elegant
			"""
			entry['id'], is_new = cls.get_id(entry)
			entry['name'] = cls.get_name(entry)
			entry['dir'] = cls.get_path(entry)
			dump_location = path_pfx+entry['dir']
			makedir_if_not_exists(dump_location)
			logging.debug('dest dir: {}'.format(dump_location))

			with benchmark('', suppress=True) as b:
				entry['size'] = dump_df(df, entry['name'], dir_path=dump_location)

			entry['dumptime'] = round(b.delta.total_seconds(), 2)
			entry['hash'] = sum(hash_pandas_object(df))
			addition = pd.DataFrame(columns=DR_COLS, index=[entry['id']])

			if (is_new):
				entry['created'] = str_now()
				addition.loc[entry['id']] = entry
				cls.DATA_RECORD = pd.concat([cls.DATA_RECORD, addition], copy=False)
			else:
				entry['modified'] = str_now()
				addition.loc[entry['id']] = entry
				cls.DATA_RECORD.update(addition)

			if (update_record):
				cls.dump_record()

	@classmethod
	def initialize(cls):
		try:
			cls.DataRecordAPI.reload_record()
		except FileNotFoundError as e:
			cls.DataRecordAPI.reset_record()
			logging.warning('DataAPI initialize: Data record not found, loading empty record')

	@classmethod
	def print_record(cls):
		print(cls.DataRecordAPI.get_record_view())

	@classmethod
	def generate(cls, search_dict, direct_query=False, **kwargs):
		"""Provide generator interface to get data"""
		yield from map(cls.DataRecordAPI.loader(**kwargs), cls.DataRecordAPI.matched(search_dict, direct_query=direct_query))

	@classmethod
	def get_rec_matches(cls, search_dict, direct_query=False, **kwargs):
		"""Provide generator to get matched records"""
		yield from cls.DataRecordAPI.matched(search_dict, direct_query=direct_query)

	@classmethod
	def get_df_from_rec(cls, rec, col_subsetter=None, path_pfx=DATA_DIR, **kwargs):
		sel = load_df(rec.name, dir_path=path_pfx+rec.dir, dti_freq=rec.freq, **kwargs)
		return sel if (isnt(col_subsetter)) else sel.loc[:, chained_filter(sel.columns, col_subsetter)]

	@classmethod
	def axe_yield(cls, axe, lazy=False, pfx_keys=['root'], sfx_keys=['desc']):
		"""
		Yield the data returned by querying the data record with the axefile.

		Args:
			axe (list): two item list identifying the axefile
			lazy (bool): whether or not to delay the dataframe loading
			pfx_keys (list): record entry fields to prepend to each keychain
			sfx_keys (list): record entry fields to append to each keychain

		Yields:
			A tuple (keychain, record, df) consisting of:
			* keychain (list): A list corresponding to the df consisting of [pfx_keys, axe, subset, sfx_keys]
			* record (NamedTuple): The record corresponding to the dataframe in the record
			* df (pd.DataFrame|dask.delayed): df or dask.delayed df
		"""
		for name, (df_searcher, col_subsetter) in axe_join(*map(axe_process, axe_get(axe))).items():
			for rec in cls.get_rec_matches(df_searcher):
				pfx, sfx, sub = [getattr(rec, key) for key in pfx_keys], [getattr(rec, key) for key in sfx_keys], [name]
				res = delayed(cls.get_df_from_rec)(rec, col_subsetter) if (lazy) else cls.get_df_from_rec(rec, col_subsetter)
				yield axe_get_keychain(pfx, axe, sub, sfx), rec, res

	@classmethod
	def axe_load(cls, axe, lazy=False, pfx_keys=['root'], sfx_keys=['desc']):
		"""
		Return the data returned by querying the data record with the axefile as two NDDs.
		Most of the actual functionality is in axe_yield.

		Args:
			axe (list): two item list identifying the axefile
			lazy (bool): whether or not to delay the dataframe loading
			pfx_keys (list): record entry fields to prepend to each keychain
			sfx_keys (list): record entry fields to append to each keychain

		Returns:
			A tuple (recs, dfs) consisting of:
			* recs (NestedDefaultDict): a mapping of keychains to records
			* dfs (NestedDefaultDict): a mapping of keychains to dfs or dask.delayed dfs
		"""
		recs, dfs = NestedDefaultDict(), NestedDefaultDict()
		for kc, rec, df in axe_yield(cls, axe, lazy=lazy, pfx_keys=pfx_keys, sfx_keys=sfx_keys):
			recs[kc], dfs[kc] = rec, df
		return recs, dfs

	@classmethod
	def get_search_dicts(cls, end_dg, end_cs=None, how='subsets', subset=None):
		"""
		XXX - deprecated
		Convenience function to construct the search dict (optionally col subsetter dict) based on the how parameter.
		"""
		if (how == 'all'):
			cs_dict = None if (end_cs is None) else end_cs['all']
			return {'all': (end_dg['all'], cs_dict)}

		elif (how == 'subsets'):
			ss_dict = {}

			if (isinstance(subset, list)):
				subsets_dict = {key: val for key, val in end_dg['subsets'].items() if (key in subset)}
			else:
				subsets_dict = end_dg['subsets'] # all subsets

			for s_name, s_dict in subsets_dict.items():
				cs_dict = None if (end_cs is None) else end_cs['subsets'][s_name]
				ss_dict[s_name] = (dict(end_dg['all'], **s_dict), cs_dict)

			return ss_dict

	@classmethod
	def load_from_dg(cls, df_getter, col_subsetter=None, separators=['root'], how='subsets', subset=None, **kwargs):
		"""
		XXX - deprecated
		Load data using a df_getter dictionary, and col_subsetter dictionary (optional)
		By default separates the search by root at the bottom level.
		"""
		hit_bottom = lambda val: any(key in val for key in ['all', 'subsets'])
		paths_to_end = list(dict_path(df_getter, stop_cond=hit_bottom))
		result, recs = recursive_dict(), recursive_dict()
		result_paths = []

		for edg_path, edg in paths_to_end:
			ecs = None if (col_subsetter is None) else list_get_dict(col_subsetter, edg_path)

			for sd_name, sd_cs in cls.get_search_dicts(edg, end_cs=ecs, how=how, subset=subset).items():
				sd_path = edg_path + [sd_name]
				add_desc_as_path_identifier = True if ('desc' in sd_cs[0] and isinstance(sd_cs[0]['desc'], list)) else False

				for rec, df in cls.generate(sd_cs[0]):
					seps = [getattr(rec, separator) for separator in separators]
					df_path = seps + sd_path + [rec.desc] if (add_desc_as_path_identifier) else seps + sd_path

					result_paths.append(df_path)
					filtered_df = df if (sd_cs[1] is None) else df.loc[:, chained_filter(df.columns, sd_cs[1])]
					list_set_dict(result, df_path, filtered_df)
					list_set_dict(recs, df_path, rec)

		return result_paths, recs, result


	@classmethod
	def lazy_load(cls, df_getter, col_subsetter, separators=['root'], how='subsets', subset=None, **kwargs):
		"""
		XXX - deprecated
		Return paths, rec dicts, and df dicts identically to load_from_dg, but defer final loading of DataFrames using dask.delayed.
		"""
		hit_bottom = lambda val: any(key in val for key in ['all', 'subsets'])
		end_paths = list(dict_path(df_getter, stop_cond=hit_bottom))
		result, recs = recursive_dict(), recursive_dict()
		result_paths = []

		for end_path, end_dg in end_paths:
			end_cs = None if (col_subsetter is None) else list_get_dict(col_subsetter, end_path)
			search_dicts = cls.get_search_dicts(end_dg, end_cs=end_cs, how=how, subset=subset)

			for name, search_dict_col_subsetter in search_dicts.items():
				path = end_path + [name]
				search_dict, sd_col_subsetter = search_dict_col_subsetter

				# If desc is in search_dict and refers to a list of desc values, use these to distinguish items
				desc_in_path = True if ('desc' in search_dict and isinstance(search_dict['desc'], list)) else False

				for rec in cls.get_rec_matches(search_dict):
					seps = [getattr(rec, separator) for separator in separators]
					df_path = seps + path + [rec.desc] if (desc_in_path) else seps + path
					result_paths.append(df_path)
					list_set_dict(result, df_path, delayed(cls.get_df_from_rec)(rec, sd_col_subsetter))
					list_set_dict(recs, df_path, rec)

		return result_paths, recs, result

	@classmethod
	def lazy_yield(cls, df_getter, col_subsetter, separators=['root'], how='subsets', subset=None, **kwargs):
		"""
		XXX - deprecated
		Yield paths, rec dicts, and df dicts, but defer final loading of DataFrames using dask.delayed.
		"""
		hit_bottom = lambda val: any(key in val for key in ['all', 'subsets'])
		end_paths = list(dict_path(df_getter, stop_cond=hit_bottom))

		for end_path, end_dg in end_paths:
			end_cs = None if (col_subsetter is None) else list_get_dict(col_subsetter, end_path)
			search_dicts = cls.get_search_dicts(end_dg, end_cs=end_cs, how=how, subset=subset)

			for name, search_dict_col_subsetter in search_dicts.items():
				path = end_path + [name]
				search_dict, sd_col_subsetter = search_dict_col_subsetter

				# If desc is in search_dict and refers to a list of desc values, use these to distinguish items
				desc_in_path = True if ('desc' in search_dict and isinstance(search_dict['desc'], list)) else False

				for rec in cls.get_rec_matches(search_dict):
					seps = [getattr(rec, separator) for separator in separators]
					df_path = seps + path + [rec.desc] if (desc_in_path) else seps + path

					yield df_path, rec, delayed(cls.get_df_from_rec)(rec, sd_col_subsetter)

	@classmethod
	def get_record_view(cls):
		return cls.DataRecordAPI.get_record_view()

	@classmethod
	def dump(cls, df, entry, **kwargs):
		cls.DataRecordAPI.dump(df, entry, **kwargs)

	@classmethod
	def update_record(cls):
		cls.DataRecordAPI.dump_record()


DataAPI.initialize()
